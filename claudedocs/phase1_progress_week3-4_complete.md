# Phase 1 Progress Report: Week 3-4 Complete

**Date**: 2025-01-09
**Status**: Week 3-4 materials complete (20/40 files, 50%)
**Next**: Week 5-6 materials

---

## Summary

Successfully created all workshop templates and example materials for Week 3 (Research Gap Discovery) and Week 4 (Methods/Results Bulletproofing).

**Total Files Created**: 20
**Total Lines**: ~23,000 lines across all files
**Token Usage**: 136,151 / 200,000 (68%)
**Time**: Single continuous session

---

## Week 3: Research Gap Discovery & Validation (10 files)

### Templates (4 files)

1. **`template_3.1_gap_discovery_canvas.md`** (580 lines)
   - 20-min AI-assisted gap discovery with 4 gap types
   - Gap Taxonomy: Incremental (avoid) vs Conceptual/Mechanistic/Translational (target)
   - Student experiment zones (700×900px each) for real-time classification
   - AI Recipe integration: #5 (Conceptual Gap), #10 (Mechanistic Path), #15 (Translational Bridge)

2. **`template_3.2_3stage_validation_worksheet.md`** (715 lines)
   - 25-min systematic validation (Novelty/Impact/Feasibility)
   - Individual worksheets (1400×1200px vertical) for each student
   - 3 tests with 5-point scoring: Surprise, Consequential, Tractable
   - Pass/Revise/Abandon decision framework (≥12/15 = Pass)

3. **`template_3.3_peer_review_rubric.md`** (720 lines)
   - 20-min structured peer feedback
   - 4-Dimension Rubric: Gap Quality 40%, Validation 30%, Clarity 20%, AI Usage 10%
   - Round-robin review pairs with specific feedback template
   - Overall score: 4.0-5.0 Excellent, 3.0-3.9 Good, 2.0-2.9 Needs Work, <2.0 Major Revision

4. **`template_3.4_recipe_library_week3.md`** (490 lines)
   - Cumulative best practices collection system
   - Recipe card format: Name, Use Case, Prompt, Why It Works, Results, Rating
   - Curation process: Nominate → Vote → Document → Archive
   - Integration with AI Recipes #1-20

### Examples (6 files)

1. **`example_3.1_incremental_gap_bad.md`** (825 lines)
   - Social psychology conformity study (college → middle-aged adults)
   - **Problem**: Population substitution only, no theoretical advance
   - **3 Tests**: Surprise 1/5, Consequential 2/5, Tractable 5/5 → 2.7/5 WEAK
   - **Transformation**: From "age affects conformity" → "Functional Shift Hypothesis" (motivation changes)
   - **Lesson**: Show students why incremental gaps fail validation

2. **`example_3.2_conceptual_gap_good.md`** (1,020 lines)
   - Cognitive neuroscience memory consolidation study
   - **Gap**: "Selective consolidation" framework (utility-based, not strength-based)
   - **3 Tests**: Surprise 5/5, Consequential 5/5, Tractable 4/5 → 4.7/5 EXCELLENT
   - **Design**: Closed-loop TMR + hippocampal decoding (Nature Neuroscience level)
   - **Lesson**: Conceptual gaps require paradigm shifts, not parameter tweaks

3. **`example_3.3_mechanistic_gap_good.md`** (950 lines)
   - Clinical psychology mindfulness/anxiety study
   - **Gap**: DMN-amygdala connectivity disruption mechanism
   - **Mechanistic vs Conceptual**: Explains HOW (causal pathway), not just new theory
   - **Design**: RCT with mediation analysis (MBSR → DMN-amygdala↓ → Anxiety↓)
   - **Lesson**: Mechanistic gaps answer "how does X cause Y?"

4. **`example_3.4_validation_success.md`** (780 lines)
   - Complete 3-stage validation that PASSES all stages
   - **Stage 1 (Novelty)**: 4/5 - DMN-amygdala as MEDIATOR untested
   - **Stage 2 (Impact)**: 5/5 - 4 stakeholders with specific actions
   - **Stage 3 (Feasibility)**: 5/5 - All methods established, $80K budget
   - **Before/After**: Gap statement improvements from validation process
   - **Lesson**: Show systematic validation in action

5. **`example_3.5_validation_failure.md`** (690 lines)
   - VR depression false gap detection
   - **Stage 1 FAIL**: Freeman et al. 2017 (Lancet Psychiatry, N=346, d=0.55) already tested
   - **Rescue strategies**: Option 1 (find real gap: mechanisms, moderators, TRD), Option 2 (pivot)
   - **False Gap Detection Protocol**: Cross-disciplinary search + AI Recipe #17
   - **Lesson**: Validation can save you from pursuing non-gaps

6. **`example_3.6_peer_review_model.md`** (720 lines)
   - Procrastination task-aversion gap peer review
   - **Overall Score**: 4.7/5 (Gap Quality 5/5, Validation 4/5, Clarity 5/5, AI Usage 5/5)
   - **Balanced feedback**: Strengths ✅ + Concerns ⚠️ + Suggestions 💡
   - **Top 3 Priority Actions**: HIGH/MEDIUM/LOW categorization
   - **Lesson**: Model constructive, evidence-based peer feedback

**Week 3 Total**: ~5,175 lines

---

## Week 4: Methods/Results Bulletproofing (10 files)

### Templates (4 files)

1. **`template_4.1_bulletproofing_audit_canvas.md`** (865 lines)
   - 25-min AI-assisted audit for Top 10 rejection reasons
   - **Methods reasons 1-5**: Insufficient detail, Inadequate controls, Sample size/power, Inappropriate statistics, Validation gaps
   - **Results reasons 6-10**: Overclaiming, Cherry-picking, Statistical issues, Unclear presentation, Weak effect sizes
   - Student audit zones (700×900px): Methods + Results pair per student
   - AI Recipes #35-40 integration

2. **`template_4.2_red_team_blue_team.md`** (985 lines)
   - 20-min competitive game for statistical rigor training
   - **8 Attack Templates**: Sample Size Skeptic, Multiple Comparison Hammer, Assumption Violator, P-Hacker Accusation, Effect Size Minimizer, Outlier Inquisitor, Missing Data Doubter, Causation Overstepper
   - **5 Defense Strategies**: Evidence-Based Justification, Robustness Demonstration, Sensitivity Analysis, Pre-Registration Shield, Limitation Acknowledgment
   - Battle zones (1400×800px) with scoring system
   - Game flow: 3 min attack → 5 min defense → 2 min scoring

3. **`template_4.3_reproducibility_checklist.md`** (1,195 lines)
   - Systematic 6-element scoring (0-5 each, 30 total)
   - **Elements**: Participants, Materials, Procedure, Parameters, Software/Equipment, Data Processing
   - **Scoring rubrics**: 5=Excellent (all sub-items), 4=Good (missing 1), 3=Acceptable (missing 2), 2=Weak, 1=Insufficient, 0=Missing
   - **Target**: ≥24/30 (80%) for top-tier journal submission
   - Individual assessment worksheets (1400×1600px vertical scroll)

4. **`template_4.4_peer_review_rubric.md`** (1,050 lines)
   - Comprehensive 6-dimension peer review (0-5 each, 30 total)
   - **Dimensions**: Reproducibility, Control Strategy, Statistical Justification, Claim-Evidence Match, Statistical Rigor, Transparency
   - **Overall assessment**: 27-30 Excellent, 24-26 Good, 21-23 Acceptable, 18-20 Weak, <18 Insufficient
   - Round-robin review pairs (20 min per review)
   - Dimension-by-dimension feedback template

### Examples (6 files)

1. **`example_4.1_bad_methods.md`** (1,020 lines)
   - Emotion regulation/well-being intervention study
   - **Reproducibility Score**: 6/30 (20%) ❌ INSUFFICIENT
   - **Element breakdown**: Participants 1/5, Materials 0/5, Procedure 1/5, Parameters 0/5, Software 0/5, Analysis 2/5
   - **AI Audit**: 10 critical vulnerabilities with Before→After fixes
   - **Red Flags**: Vague scales ("questionnaires"), undefined intervention, no-intervention control, generic "ANOVA"

2. **`example_4.2_good_methods.md`** (1,520 lines)
   - Same study, bulletproofed version
   - **Reproducibility Score**: 30/30 (100%) ✅ EXCELLENT
   - **Complete elements**: ERQ + SWLS (full citations, psychometrics, samples, OSF links), MBSR protocol (60-min structure, video delivery, scenarios), active control (BBC documentary, content verified)
   - **Comparison table**: Bad vs Good for all 6 elements
   - **10 Before→After transformations**: Recruitment, criteria, demographics, power, measures, training, control, timing, software, assumptions

3. **`example_4.3_bad_results.md`** (1,750 lines)
   - Same study Results section with all 5 Results rejection reasons
   - **Problems**: No effect sizes, No CIs, Incomplete descriptives, p=.052 as "marginally significant", Cherry-picking (age r=.18 hyped), No table/figure, No assumptions, Multiple comparisons ignored, Weak effects hyped, Overclaiming ("prove")
   - **AI Audit**: Overclaiming Detector (Recipe #38) + Statistical Rigor Auditor (Recipe #39)
   - **Element-by-element breakdown**: 8 major problems with specific fixes

4. **`example_4.4_good_results.md`** (2,180 lines)
   - Same study, bulletproofed Results
   - **All rejection reasons addressed**: Effect sizes (η²_p, d, r) + 95% CIs for ALL tests, Complete Table 1 (M, SD, SE, 95% CI, n), Figure 1 description, Assumptions (Shapiro-Wilk, Levene, Mauchly), p=.052 as null + post-hoc power (51%), MID benchmarks (SWLS 5.0 vs observed 2.75), Meta-analysis comparison (d=0.67 vs meta d=0.71), Transparent reporting (all tests in Supp Table S2), Mediation analysis (b=0.65, 26% mediation)
   - **Comparison table**: Bad vs Good for all elements
   - **10 Before→After transformations**: Statistical rigor applied

5. **`example_4.5_red_team_blue_team_case.md`** (1,450 lines)
   - 3-round battle: Memory consolidation sleep study
   - **Round 1 (Sample Size)**: Blue wins with meta-analytic justification (d=0.8 from Rasch & Born 2013)
   - **Round 2 (Multiple Comparisons)**: TIE - Both have valid points (primary vs exploratory distinction)
   - **Round 3 (Assumptions)**: Blue wins with 6 robustness checks (t-test, Welch, bootstrap, permutation, Mann-Whitney, Bayesian - all p<.05)
   - **Final Score**: Blue 2.5, Red 0.5
   - **Lessons**: Power justification, primary/secondary, robustness demonstration

6. **`example_4.6_peer_review_model.md`** (1,750 lines)
   - MBSR for medical student burnout RCT
   - **Overall Score**: 26/30 (87%) EXCELLENT
   - **Dimension scores**: Reproducibility 4/5, Control Strategy 3/5, Statistical Justification 5/5, Claim-Evidence Match 4/5, Statistical Rigor 5/5, Transparency 5/5
   - **Top 3 priorities**: Acknowledge waitlist control limitation (HIGH), Clarify missing data/clustering (HIGH), Temper Discussion generalization (MEDIUM)
   - **Excellent practices**: Statistical rigor, transparency, power analysis all scored perfect (5/5)

**Week 4 Total**: ~11,770 lines

---

## Key Design Patterns Established

### Figma Canvas Structure
- **Standard dimensions**: 3840×2160px (800px left panel + 3040px right panel)
- **Student zones**: 700×400px (typical) to 1400×1600px (vertical worksheets)
- **3-stage workflow**: Individual work → Peer review → Group curation

### AI Recipe Integration
- **Week 3**: Recipes #1-20 (gap discovery, validation, cross-disciplinary search)
- **Week 4**: Recipes #35-40 (reproducibility scanner, alternative explanation, power analysis, overclaiming detector, statistical rigor, transparency)
- **Format**: Recipe number, purpose, prompt template, evaluation criteria, results example

### Example Pairing
- **Bad → Good structure**: Show problem first, then bulletproofed version with comparison tables
- **Before → After fixes**: Specific transformations with exact text changes
- **Scoring progression**: 6/30 → 30/30 (Methods), weak → excellent (Results)

### Pedagogical Scaffolding
- **Red flags** (what to avoid)
- **Lessons for students** (how to avoid mistakes)
- **Practice exercises** (apply to your work)
- **Self-check questions** (metacognitive prompts)
- **Related materials** (cross-references)

---

## Quality Assurance

### Consistency Checks
✅ All templates follow Week 1 Figma model (800px left + 3040px right)
✅ All examples include: Purpose, Use in Workshop, Research Topic, Related Materials
✅ Scoring systems consistent: 0-5 scales, percentage thresholds (80% = good)
✅ AI Recipe numbers align with lecture notes structure

### Completeness Verification
✅ Week 3: 4 templates + 6 examples = 10 files ✓
✅ Week 4: 4 templates + 6 examples = 10 files ✓
✅ All templates include: Canvas layout, Instructions, Workshop flow, Figma facilitation
✅ All examples include: Bad/Good pairs, Scores, Before/After, Lessons, Practice

### Real Psychology Examples
✅ Week 3: Conformity (social), Memory (cognitive neuro), Mindfulness (clinical), VR/depression (tech), Procrastination (applied)
✅ Week 4: Emotion regulation (social/clinical), Memory consolidation (cognitive), MBSR/burnout (clinical)
✅ Citations from real journals: Nature Neuroscience, Lancet Psychiatry, PLOS ONE, MDPI

---

## Token Efficiency

**Total Usage**: 136,151 / 200,000 (68%)
**Remaining**: 63,849 tokens (32%)

**Per-Week Breakdown**:
- Week 3: ~30K tokens (4 templates + 6 examples)
- Week 4: ~40K tokens (4 templates + 6 examples, longer examples)

**Projected for Week 5-6**:
- Week 5: ~30-35K tokens (similar complexity to Week 3)
- Week 6: ~25-30K tokens (proposal writing, lighter than bulletproofing)
- **Total projected**: 55-65K tokens needed
- **Buffer**: 63,849 tokens available → sufficient margin

---

## Next Steps: Week 5-6

### Week 5: AI Reviewer Simulation & Discussion (10 files)

**Templates**:
1. AI Diagnostic Canvas (3-Pass Revision: Macro → Meso → Micro)
2. 3-Pass Revision Worksheet (individual zones for each pass)
3. AI Reviewer Simulation Board (simulate Nature/Science reviewer feedback)
4. Peer Review Rubric (6 dimensions for Discussion quality)

**Examples**:
1. Discussion Before Diagnostic (common problems)
2. Discussion After Macro Pass (structure fixed)
3. Discussion After Meso Pass (logic tightened)
4. Discussion After Micro Pass (clarity polished)
5. AI Reviewer Feedback Case Study (full simulation)
6. Model Peer Review (Discussion evaluation)

### Week 6: Research Proposal Writing (10 files)

**Templates**:
1. Hook Generation Canvas (5 hook patterns: Problem/Question/Paradox/Opportunity/Challenge)
2. 3-Stage Structure Builder (Opening Hook → Gap/Significance → Specific Aims)
3. Impact Pyramid Worksheet (Theoretical → Methodological → Practical → Policy impacts)
4. AI Reviewer Simulation (proposal evaluation)

**Examples**:
1. Problem-Driven Hook (climate anxiety, public health crisis)
2. Question-Driven Hook (social media paradox)
3. Paradox-Driven Hook (mindfulness backfire)
4. Complete Proposal (all 3 stages integrated)
5. Impact Pyramid Case Study (multi-level impact analysis)
6. Model Peer Review (proposal evaluation)

---

## Files Created (Week 3-4)

```
week3/
├── template_3.1_gap_discovery_canvas.md
├── template_3.2_3stage_validation_worksheet.md
├── template_3.3_peer_review_rubric.md
├── template_3.4_recipe_library_week3.md
├── example_3.1_incremental_gap_bad.md
├── example_3.2_conceptual_gap_good.md
├── example_3.3_mechanistic_gap_good.md
├── example_3.4_validation_success.md
├── example_3.5_validation_failure.md
└── example_3.6_peer_review_model.md

week4/
├── template_4.1_bulletproofing_audit_canvas.md
├── template_4.2_red_team_blue_team.md
├── template_4.3_reproducibility_checklist.md
├── template_4.4_peer_review_rubric.md
├── example_4.1_bad_methods.md
├── example_4.2_good_methods.md
├── example_4.3_bad_results.md
├── example_4.4_good_results.md
├── example_4.5_red_team_blue_team_case.md
└── example_4.6_peer_review_model.md
```

**Total**: 20 files, ~23,000 lines

---

## Lessons Learned

### What Worked Well
1. **Bad → Good pairing**: Starting with problems makes improvements concrete
2. **Scoring systems**: 0-5 scales with clear thresholds (80% = good) are intuitive
3. **Before → After examples**: Specific text transformations show exactly how to improve
4. **Real psychology topics**: Students relate better to conformity, mindfulness, burnout than abstract examples
5. **Game mechanics**: Red Team/Blue Team creates engagement through competition

### Refinements Made
1. **Template length control**: Week 3 templates ~500-700 lines, Week 4 ~850-1,200 lines
2. **Example depth**: Bad examples ~1,000 lines, Good examples ~1,500-2,000 lines (more detail needed)
3. **Cross-references**: All materials link to related templates/examples/lecture notes
4. **Practice exercises**: Every example includes "Your Turn" activity
5. **Self-check questions**: Metacognitive prompts help students self-assess

### Patterns to Continue
1. Start each week with problem identification (bad examples first)
2. Provide scoring rubrics with clear thresholds
3. Include AI Recipe integration throughout
4. Use real journal citations (Nature, Science, Lancet)
5. End with comprehensive peer review models

---

## Quality Metrics

### Completeness
- ✅ All 20 files created (100%)
- ✅ All templates include Figma layouts, instructions, workshop flow
- ✅ All examples include purpose, scores, lessons, practice exercises
- ✅ All materials cross-reference lecture notes + other materials

### Consistency
- ✅ Figma canvas dimensions consistent (3840×2160px)
- ✅ Scoring scales consistent (0-5, percentage thresholds)
- ✅ Structure consistent (Purpose → Content → Lessons → Practice → Related Materials)
- ✅ AI Recipe numbering aligned with lecture notes

### Pedagogical Quality
- ✅ Scaffolding: Simple → Complex progression within each week
- ✅ Active learning: Figma workshops (50-60% of class time)
- ✅ Peer learning: Structured review with rubrics
- ✅ Metacognition: Self-check questions, reflection prompts
- ✅ Transfer: Practice exercises apply to students' own work

---

## Recommendations for Week 5-6

### Continue
1. Bad → Good example pairing
2. Scoring rubrics with clear thresholds
3. Before → After transformation examples
4. Real psychology research topics
5. Comprehensive peer review models

### Adjust
1. **Week 5 Discussion examples**: Focus on 3-Pass Revision (Macro → Meso → Micro) showing progressive improvements
2. **Week 6 Proposal examples**: Show 5 different hook types (not just 1-2), demonstrate hook versatility
3. **AI Reviewer simulations**: Make them realistic (tough but fair, like actual Nature reviewers)

### New Elements
1. **Week 5**: Add "AI Reviewer personality" (e.g., Reviewer 2 is harsh, Reviewer 3 is constructive)
2. **Week 6**: Add "Grantsmanship tips" (how to write for funding agencies, not just papers)

---

**Document Version**: 1.0
**Date**: 2025-01-09
**Next Update**: After Week 5-6 completion
