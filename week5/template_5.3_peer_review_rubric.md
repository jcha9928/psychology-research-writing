# Week 5 - Template 5.3: Peer Review Rubric (Top 5% Checklist)

**Workshop Duration**: 15 minutes
**Objective**: Provide structured peer feedback using top-tier journal standards
**Canvas Size**: 3840×2160px (Figma collaborative canvas)

---

## Canvas Structure

```
┌─────────────────────────────────────────────────────────────────────────────┐
│ LEFT PANEL (800×2160px) - TOP 5% CRITERIA & RATING SCALES                 │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│ 🎯 NOVELTY (20%)          - Conceptual gap vs incremental                  │
│ 🔬 RIGOR (30%)            - Reproducibility, controls, statistics          │
│ 💎 CLARITY (20%)          - Message, logic, figures                        │
│ 🌍 IMPACT (20%)           - Broader significance, citations                │
│ ✨ PRESENTATION (10%)     - Zero errors, professional quality              │
│                                                                              │
│ [Detailed rubrics for each criterion]                                       │
│ [0-5 scale definitions]                                                     │
│ [Decision categories: Accept/Minor/Major/Reject]                            │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ RIGHT PANEL (3040×2160px) - PEER REVIEW ZONES                              │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  REVIEW ZONE A (1400×2160px)                 REVIEW ZONE B (1400×2160px)   │
│  ┌──────────────────────────────┐           ┌──────────────────────────┐  │
│  │ Peer Review #1                │           │ Peer Review #2           │  │
│  │ Student evaluates Paper A     │           │ Student evaluates Paper B│  │
│  │                                │           │                          │  │
│  │ - 5-dimension scoring          │           │ - 5-dimension scoring    │  │
│  │ - Strengths (1-2)              │           │ - Strengths (1-2)        │  │
│  │ - Weaknesses (3)               │           │ - Weaknesses (3)         │  │
│  │ - Concrete suggestions (3-5)   │           │ - Concrete suggestions   │  │
│  │ - Decision + rationale         │           │ - Decision + rationale   │  │
│  │                                │           │                          │  │
│  └──────────────────────────────┘           └──────────────────────────┘  │
│                                                                              │
│  REFLECTION ZONE (3040×400px)                                               │
│  ┌────────────────────────────────────────────────────────────────────┐    │
│  │ META-REFLECTION: What did I learn from reviewing others' papers?   │    │
│  │ Common patterns, successful strategies, mistakes to avoid           │    │
│  └────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Pedagogical Rationale

### Why Structured Peer Review?

**Problem with Unstructured Feedback**:
- ❌ Too vague: "This is good" or "Improve clarity"
- ❌ Too harsh: "This is terrible" without constructive guidance
- ❌ Too narrow: "Fix this typo" while missing major flaws
- ❌ Inconsistent: Different standards for different papers

**Solution: Top 5% Rubric**:
- ✅ **Explicit criteria** - everyone evaluates same dimensions
- ✅ **Calibrated standards** - Nature/Science level benchmarks
- ✅ **Balanced feedback** - requires both strengths AND weaknesses
- ✅ **Actionable suggestions** - specific improvements, not generic advice
- ✅ **Decision rationale** - practice thinking like a journal editor

### Learning Outcomes:

1. **Evaluative judgment** - ability to assess quality objectively
2. **Peer learning** - see what works in others' writing
3. **Self-awareness** - recognize own mistakes in others' papers
4. **Constructive criticism** - give helpful feedback, not just negative
5. **Editorial thinking** - understand how reviewers make decisions

---

## LEFT PANEL: Top 5% Criteria & Rating Scales

### 🎯 NOVELTY (20% weight) - Is This a Conceptual Contribution?

**What This Measures**:
- Gap quality: Conceptual/Mechanistic vs Incremental
- Surprise factor: Predictable vs Unexpected findings
- Theoretical advance: Does this change how we think?

**Rating Scale (0-5)**:

| Score | Label | Description | Example |
|-------|-------|-------------|---------|
| **5** | **Groundbreaking** | Novel theory/mechanism that challenges assumptions. Nature/Science level. | New framework explaining paradox in field |
| **4** | **Strong** | Conceptual gap with clear theoretical advance. Top journal tier. | Mechanistic pathway for known effect |
| **3** | **Moderate** | Good question, incremental advance. Mid-tier journal. | New population with theoretical justification |
| **2** | **Weak** | Incremental, predictable from existing work. Low-tier journal. | Population substitution, no new theory |
| **1** | **Very Weak** | No clear gap or trivial gap. Unlikely to publish. | "We haven't tested X in Y population" (arbitrary) |
| **0** | **None** | No identifiable gap or already answered. Reject. | Replication without novel angle |

**Key Questions**:
- [ ] Is the gap conceptual (theory/mechanism) or incremental (population/method)?
- [ ] Would findings surprise experts in the field?
- [ ] Does this advance theory or just add data?
- [ ] Is the "So what?" question answered compellingly?

**Red Flags** (score ≤2):
- Gap is "no one has tested X in Y population" without theoretical justification
- Findings are entirely predictable from existing theory
- Introduction lists what we don't know, but not why it matters
- "We replicated X" without explaining what's gained

**Green Flags** (score ≥4):
- Gap challenges current theoretical assumptions
- Findings would be surprising/counterintuitive if true
- Proposes new framework or mechanism
- Connects previously separate literatures

---

### 🔬 RIGOR (30% weight) - Can I Trust These Results?

**What This Measures**:
- Reproducibility: Can others replicate this?
- Control strategy: Are alternative explanations ruled out?
- Statistical justification: Appropriate power, methods, transparency
- Methodological quality: Professional-grade execution

**Rating Scale (0-5)**:

| Score | Label | Description | Reproducibility Score |
|-------|-------|-------------|----------------------|
| **5** | **Exemplary** | Gold standard methods, all details present, pre-registered. | 30/30 |
| **4** | **Strong** | High rigor, minor details missing, good controls. | 26-29/30 |
| **3** | **Adequate** | Acceptable methods, some gaps in detail or controls. | 21-25/30 |
| **2** | **Weak** | Significant methodological flaws or missing info. | 15-20/30 |
| **1** | **Poor** | Major flaws (no power analysis, no controls, selective reporting). | 10-14/30 |
| **0** | **Unacceptable** | Fatal methodological problems. Cannot trust results. | <10/30 |

**Key Checklist** (from Week 4 Bulletproofing):

**Reproducibility (6 elements)**:
- [ ] Participants: Detailed recruitment, demographics, inclusion/exclusion
- [ ] Materials: Measures specified (versions, reliability, validity, sample items)
- [ ] Procedure: Step-by-step protocol, timing, counterbalancing
- [ ] Parameters: All settings (trials, stimuli, durations)
- [ ] Software: Analysis scripts, packages, versions
- [ ] Data Processing: Exclusions, outliers, transformations justified

**Control Strategy**:
- [ ] Alternative explanations identified
- [ ] Appropriate control condition (not just "no treatment")
- [ ] Blinding (participants, experimenters, scorers)
- [ ] Confounds addressed (order effects, demand characteristics)

**Statistical Rigor**:
- [ ] Power analysis (a priori or meta-analytic)
- [ ] Assumptions checked (normality, homogeneity)
- [ ] Multiple comparisons corrected
- [ ] Effect sizes + 95% CIs for all tests
- [ ] Transparent reporting (all results, not selective)

**Red Flags** (score ≤2):
- No power analysis or justification for sample size
- Control condition inadequate (waitlist only, no sham)
- Missing critical details (e.g., "questionnaires" without specifying which)
- p-hacking indicators (p=.052 as "marginally significant")
- Selective reporting (only significant results)

**Green Flags** (score ≥4):
- Pre-registered hypotheses and analysis plan
- Power analysis with meta-analytic effect size
- Multiple control conditions
- Robustness checks (sensitivity analyses)
- All materials/data publicly available (OSF, GitHub)

---

### 💎 CLARITY (20% weight) - Is the Message Clear?

**What This Measures**:
- Core message: Can I summarize the paper in one sentence?
- Logical flow: Does Introduction → Methods → Results → Discussion make sense?
- Figure/Table quality: Self-explanatory and essential
- Writing quality: Clear, concise, professional

**Rating Scale (0-5)**:

| Score | Label | Description | Reader Experience |
|-------|-------|-------------|-------------------|
| **5** | **Crystal Clear** | One coherent story, effortless to follow. | "I get it after one read" |
| **4** | **Clear** | Minor confusion, but overall message is strong. | "Clear after careful read" |
| **3** | **Acceptable** | Some logical gaps or unclear sections. | "Had to re-read parts" |
| **2** | **Unclear** | Message is muddled, logic is hard to follow. | "Confused about main point" |
| **1** | **Confusing** | No clear message, disjointed sections. | "Can't summarize the paper" |
| **0** | **Incomprehensible** | Unreadable due to poor writing or logic. | "Gave up trying to understand" |

**Key Checklist**:

**Message Coherence**:
- [ ] Can I state the main finding in one sentence?
- [ ] Do Title, Abstract, and Discussion conclusion align?
- [ ] Is the research question clear?

**Logical Flow**:
- [ ] Introduction sets up the research question
- [ ] Methods address the question
- [ ] Results answer the question
- [ ] Discussion interprets the answer
- [ ] No logical jumps or missing steps

**Figure/Table Quality**:
- [ ] All figures/tables are necessary (not redundant with text)
- [ ] Captions are self-explanatory
- [ ] Figures tell a story (can understand without reading text)
- [ ] Professional design (clear labels, appropriate scales)

**Writing Quality** (Week 1 principles):
- [ ] Subject-verb proximity (≤10 words apart)
- [ ] Old→New information flow in paragraphs
- [ ] Concise (no unnecessary words)
- [ ] Active voice where appropriate

**Red Flags** (score ≤2):
- Can't identify the main research question
- Introduction and Discussion seem to be about different papers
- Figures are confusing or unnecessary
- Paragraphs jump between unrelated topics
- Excessive jargon without definitions

**Green Flags** (score ≥4):
- Main message is immediately clear from Abstract
- Story flows naturally from Introduction to Discussion
- Figures are beautiful and tell the story at a glance
- Writing is crisp and professional (Week 1 principles applied)

---

### 🌍 IMPACT (20% weight) - Does This Matter?

**What This Measures**:
- Broader significance: Interests beyond narrow subfield
- Theoretical impact: Changes how we think about phenomenon
- Practical impact: Applications or translational potential
- Citation potential: Will others build on this?

**Rating Scale (0-5)**:

| Score | Label | Description | Likely Impact Factor |
|-------|-------|-------------|---------------------|
| **5** | **High Impact** | Multi-disciplinary interest, paradigm-shifting. | Nature/Science (IF >40) |
| **4** | **Strong Impact** | Broad subfield interest, influential. | Top specialty (IF 10-20) |
| **3** | **Moderate Impact** | Specialized but solid contribution. | Mid-tier (IF 3-6) |
| **2** | **Limited Impact** | Narrow interest, incremental. | Low-tier (IF 1-3) |
| **1** | **Minimal Impact** | Very niche, unlikely to be cited much. | (IF <1) |
| **0** | **No Impact** | No clear contribution to field. | Unpublishable |

**Key Questions**:
- [ ] Will researchers outside this subfield care?
- [ ] Does this resolve a longstanding debate or paradox?
- [ ] Are there applied/translational implications?
- [ ] Will this generate follow-up studies?
- [ ] Does this open new research directions?

**Impact Indicators** (score ≥4):
- **Theoretical**: Resolves contradictions, proposes new framework
- **Methodological**: New technique that others will adopt
- **Applied**: Clinical, educational, or policy implications
- **Interdisciplinary**: Bridges multiple fields (e.g., psychology + neuroscience)

**Red Flags** (score ≤2):
- Only interesting to 10 people in the world who work on this exact topic
- "Future research should..." is the only contribution
- No clear implication for theory or practice
- Confirmatory result with no new angle

**Green Flags** (score ≥4):
- Multiple fields could cite this (psych, neuroscience, education, etc.)
- Challenges dominant theory or common assumption
- Has translational pathway (lab → clinic or policy)
- Opens new research questions

---

### ✨ PRESENTATION (10% weight) - Is This Professional?

**What This Measures**:
- Grammar/spelling: Zero errors
- APA format: Perfect adherence to APA 7th edition
- Figure/table professionalism: Publication-ready quality
- Overall polish: Attention to detail

**Rating Scale (0-5)**:

| Score | Label | Description | Error Count |
|-------|-------|-------------|-------------|
| **5** | **Perfect** | Zero errors, professional quality throughout. | 0 errors |
| **4** | **Excellent** | 1-2 minor errors, otherwise flawless. | 1-2 errors |
| **3** | **Good** | 3-5 errors, acceptable quality. | 3-5 errors |
| **2** | **Poor** | 6-10 errors, needs proofreading. | 6-10 errors |
| **1** | **Unacceptable** | >10 errors, careless. | 11-20 errors |
| **0** | **Unprofessional** | Pervasive errors, not submission-ready. | >20 errors |

**Error Types to Count**:

**Grammar/Spelling** (1 error each):
- Subject-verb agreement ("The data *was*..." → "The data *were*...")
- Tense inconsistency
- Article errors (a/an/the)
- Spelling mistakes
- Punctuation errors (comma splices, missing commas)
- Apostrophe errors

**APA Format** (1 error each):
- In-text citation format (Smith & Jones, 2020) vs (Smith and Jones, 2020)
- Et al. usage (3+ authors after first mention)
- Reference format (author, year, title, source, DOI)
- Heading levels (Level 1, 2, 3)
- Table/Figure format (title above table, caption below figure)

**Figure/Table Quality**:
- [ ] Axes labeled clearly
- [ ] Legends present and informative
- [ ] High resolution (not pixelated)
- [ ] Error bars explained in caption
- [ ] Professional design (not default Excel)

**Overall Polish**:
- [ ] Consistent formatting throughout
- [ ] No placeholder text ("INSERT CITATION")
- [ ] No orphan headings (heading at bottom of page)
- [ ] No widows (single line at top/bottom of page)

**Target for Top 5%**: Score 5/5 (zero errors) - Top journals desk reject for careless errors

---

## RIGHT PANEL: Peer Review Zones

### Structure: Each Student Reviews 2 Peers' Papers

**Why 2 Reviews?**
- **Reliability**: One reviewer might be too harsh/lenient - two provides balance
- **Triangulation**: Student gets feedback from 2 peers + AI reviewers (3 total)
- **Reciprocity**: If 10 students each review 2, everyone gets ~2 reviews (round-robin)
- **Cognitive load**: Reviewing 2 papers thoroughly > 3 papers superficially

**Assignment Method** (Instructor):
1. **Randomize pairs**: Student A → reviews Papers B & C
2. **Avoid reciprocal only**: Don't let A review B if B reviews A only (need triangulation)
3. **Match by topic** (optional): Group similar research areas for more informed feedback

---

### REVIEW ZONE TEMPLATE (for each of 2 reviews)

```
╔═══════════════════════════════════════════════════════════════════════╗
║                        PEER REVIEW #1                                  ║
╚═══════════════════════════════════════════════════════════════════════╝

📄 PAPER INFO:
- Author (anonymous ID): ___
- Title: ___
- Target journal (if stated): ___

═══════════════════════════════════════════════════════════════════════

📊 TOP 5% EVALUATION (0-5 for each dimension)

🎯 NOVELTY (20% weight): ___/5
   Gap quality: [Conceptual / Mechanistic / Incremental]
   Justification: ___

   Strengths:
   • ___

   Concerns:
   • ___

🔬 RIGOR (30% weight): ___/5
   Reproducibility estimate: ___/30
   Control strategy: [Excellent / Good / Adequate / Weak]
   Statistical rigor: [Strong / Acceptable / Concerns]

   Strengths:
   • ___

   Concerns:
   • ___

💎 CLARITY (20% weight): ___/5
   Main message (in 1 sentence): "___"
   Logical flow: [Smooth / Minor gaps / Confusing]
   Figure/Table quality: [Excellent / Good / Adequate / Poor]

   Strengths:
   • ___

   Concerns:
   • ___

🌍 IMPACT (20% weight): ___/5
   Broader significance: [High / Moderate / Limited / Minimal]
   Likely citations: [Many / Some / Few / Rare]

   Strengths:
   • ___

   Concerns:
   • ___

✨ PRESENTATION (10% weight): ___/5
   Error count: ___
   APA adherence: [Perfect / Good / Needs work]

   Strengths:
   • ___

   Concerns:
   • ___

═══════════════════════════════════════════════════════════════════════

🎯 WEIGHTED TOTAL SCORE: ___/5
   Calculation:
   (Novelty×0.2) + (Rigor×0.3) + (Clarity×0.2) + (Impact×0.2) + (Presentation×0.1)

   Example: (4×0.2) + (5×0.3) + (3×0.2) + (4×0.2) + (5×0.1) = 4.2/5

═══════════════════════════════════════════════════════════════════════

📈 OVERALL ASSESSMENT:

□ TOP 5% (4.5-5.0): Exceptional - Nature/Science level
□ TOP 10% (4.0-4.4): Excellent - Top specialty journal
□ TOP 25% (3.5-3.9): Strong - Good journal
□ TOP 50% (3.0-3.4): Adequate - Mid-tier journal
□ BELOW AVERAGE (2.0-2.9): Needs substantial revision
□ WEAK (<2.0): Major problems, consider reframing

═══════════════════════════════════════════════════════════════════════

💪 GREATEST STRENGTH (pick ONE):
□ Novel conceptual framework
□ Exceptionally rigorous methods
□ Crystal-clear writing and logic
□ High impact/broad significance
□ Perfect presentation quality

Explanation (1-2 sentences): ___

═══════════════════════════════════════════════════════════════════════

⚠️ TOP 3 WEAKNESSES (prioritized):

1. [HIGH PRIORITY] ___
   Section: [Intro/Methods/Results/Disc]
   Why this matters: ___
   Suggested fix: ___

2. [MEDIUM PRIORITY] ___
   Section: ___
   Why this matters: ___
   Suggested fix: ___

3. [LOWER PRIORITY] ___
   Section: ___
   Why this matters: ___
   Suggested fix: ___

═══════════════════════════════════════════════════════════════════════

💡 CONCRETE SUGGESTIONS (3-5 actionable items):

1. [Section: ___] ___
   Example fix: "BEFORE: ___ AFTER: ___"

2. [Section: ___] ___
   Example fix: "BEFORE: ___ AFTER: ___"

3. [Section: ___] ___
   Example fix: "BEFORE: ___ AFTER: ___"

4. [Section: ___] ___
   Example fix: "BEFORE: ___ AFTER: ___"

5. [Section: ___] ___
   Example fix: "BEFORE: ___ AFTER: ___"

═══════════════════════════════════════════════════════════════════════

✅/❌ EDITORIAL DECISION:

If I were a reviewer for [target journal], I would recommend:

□ ACCEPT - Ready to publish with minor edits
□ MINOR REVISION - Strong paper, needs polishing (1-2 weeks)
□ MAJOR REVISION - Has potential, needs substantial work (2-3 months)
□ REJECT - Fatal flaws or insufficient contribution

RATIONALE (2-3 sentences):
___

═══════════════════════════════════════════════════════════════════════

🤝 CONSTRUCTIVE TONE CHECK:

□ I balanced criticism with recognition of strengths
□ All criticisms are specific (not vague "improve clarity")
□ All suggestions are actionable (not just "this is bad")
□ I explained WHY each issue matters
□ I wrote feedback I would want to receive

═══════════════════════════════════════════════════════════════════════
```

---

## REFLECTION ZONE: Meta-Learning (Bottom of Canvas)

**Purpose**: Help students extract lessons from peer review process

**Reflection Prompts**:

```
╔═══════════════════════════════════════════════════════════════════════╗
║                      META-REFLECTION                                   ║
╚═══════════════════════════════════════════════════════════════════════╝

After reviewing 2 peers' papers, reflect:

1. COMMON PATTERNS (what did both papers struggle with?):
   • ___
   • ___
   • ___

2. SUCCESSFUL STRATEGIES (what worked well that I should emulate?):
   • ___
   • ___
   • ___

3. MISTAKES TO AVOID (what did I see that I might be doing too?):
   • ___
   • ___
   • ___

4. SURPRISES (what feedback did I give that I need to apply to my own paper?):
   • ___
   • ___

5. CALIBRATION (was I too harsh, too lenient, or balanced?):
   □ Too harsh (mostly 1-2 scores)
   □ Too lenient (mostly 4-5 scores)
   □ Balanced (range of scores with justification)

   Adjustment for next review: ___

6. ACTIONABLE INSIGHTS FOR MY PAPER:
   Based on what I learned from reviewing others:
   • I should improve my ___
   • I should check for ___
   • I should strengthen ___
```

---

## Workshop Timeline (15 minutes)

**Preparation** (Before class):
- Instructor assigns pairs: Each student reviews 2 peers' papers
- Papers shared via Google Drive or email (anonymized IDs)

**In-Class Review** (15 min):

### Minutes 1-7: First Review
- Students complete Review #1 using template
- Focus on filling out all sections (scores, strengths, weaknesses, suggestions)
- Instructor circulates, answers questions

### Minutes 8-14: Second Review
- Students complete Review #2
- Faster than Review #1 (now familiar with rubric)

### Minute 15: Reflection
- Students complete Meta-Reflection
- Quick share: "What's the most common problem you saw in both papers?"

**Post-Workshop**:
- Instructor collects reviews, returns to paper authors
- Authors receive feedback from 2 peers to integrate with AI Diagnostic feedback

---

## Scoring Calibration Examples

### Example 1: High-Scoring Paper (4.3/5 Overall)

**Novelty: 5/5** - Proposes "Selective Consolidation Hypothesis" (utility-based, not strength-based) - novel theoretical framework

**Rigor: 5/5** - Reproducibility 30/30, pre-registered RCT, power analysis based on meta-analysis, all data/materials on OSF

**Clarity: 3/5** - Main message clear, but Discussion has 8 paragraphs (too long), some nominalization issues

**Impact: 4/5** - Cross-disciplinary (memory + decision-making), strong translational angle (sleep optimization)

**Presentation: 4/5** - 2 minor errors (one comma splice, one APA citation format issue)

**Weighted Total**: (5×0.2) + (5×0.3) + (3×0.2) + (4×0.2) + (4×0.1) = **4.3/5** (Top 10%)

**Decision**: Minor Revision - Excellent paper, needs Discussion trimming and error fixes

---

### Example 2: Mid-Scoring Paper (3.2/5 Overall)

**Novelty: 3/5** - Tests mindfulness in college students (incremental, but has stress-buffering hypothesis)

**Rigor: 4/5** - Good reproducibility (26/30), adequate power (0.80), but waitlist control only (no active control)

**Clarity: 3/5** - Message clear, but Introduction is 8 pages (too long), Results could be clearer

**Impact: 2/5** - Limited to college student mental health, no broader theoretical implication

**Presentation: 4/5** - 3 errors (two subject-verb agreements, one missing DOI)

**Weighted Total**: (3×0.2) + (4×0.3) + (3×0.2) + (2×0.2) + (4×0.1) = **3.2/5** (Top 50%)

**Decision**: Major Revision - Has potential, but needs stronger theoretical framing + trim Introduction

---

### Example 3: Low-Scoring Paper (2.1/5 Overall)

**Novelty: 1/5** - "No one has tested X in Y population" - pure population substitution, no theoretical gap

**Rigor: 2/5** - Reproducibility 15/30 (vague measures, no power analysis, missing details), no control group

**Clarity: 3/5** - Writing is clear, but no coherent research question (Introduction doesn't match Methods)

**Impact: 1/5** - Very narrow interest (only for researchers working on exact same topic)

**Presentation: 3/5** - 5 errors (APA citations, one grammar error, missing Figure caption)

**Weighted Total**: (1×0.2) + (2×0.3) + (3×0.2) + (1×0.2) + (3×0.1) = **2.1/5** (Below average)

**Decision**: Reject or Major Revision - Needs complete reframing (conceptual gap) + more rigorous methods

---

## Common Pitfalls & Solutions

### Pitfall 1: Student gives all 4-5 scores (too lenient)
**Symptom**: Every dimension scored 4-5, even when paper has obvious flaws
**Diagnosis**: Student is conflict-averse or doesn't understand standards
**Solution**: Instructor calibration exercise
```
"Class, let's score this example paper together.
Novelty: It tests mindfulness in college students. No new theory.
Is that 5/5 (groundbreaking) or 3/5 (moderate)?"
[Class discusses → consensus is 3/5]
```

### Pitfall 2: Student gives all 1-2 scores (too harsh)
**Symptom**: Every dimension scored 1-2, even for competent work
**Diagnosis**: Student is hypercritical or has unrealistic standards
**Solution**: Remind of scoring anchors
```
"Score 3/5 = ADEQUATE for mid-tier journal, not bad!
Score 2/5 = WEAK, major flaws that prevent publication
Score 1/5 = UNACCEPTABLE, fatal problems

Most student papers should be in 2-4 range, not 1 or 5."
```

### Pitfall 3: Vague feedback ("Improve clarity")
**Symptom**: Suggestions like "make it clearer" without specifics
**Solution**: Require before→after examples
```
"For each suggestion, give an example:
BEFORE: [quote the problematic text]
AFTER: [show how to fix it]

VAGUE: 'Improve paragraph 3'
SPECIFIC: 'Paragraph 3 lacks topic sentence.
BEFORE: Smith et al. (2020) found...
AFTER: **Mindfulness reduces anxiety via attention regulation.** Smith et al. (2020) found...'"
```

### Pitfall 4: Only criticizing, no strengths
**Symptom**: Lists 10 weaknesses, zero strengths
**Solution**: Enforce "strengths FIRST" rule
```
"Constructive feedback sandwich:
1. Start with greatest strength (required)
2. Then weaknesses (prioritized)
3. End with concrete suggestions (actionable)

Every paper has SOME strength - find it!"
```

### Pitfall 5: Can't decide between Minor/Major Revision
**Symptom**: Student unsure if issues are fixable quickly or need months
**Solution**: Decision tree
```
MINOR REVISION (1-2 weeks):
- Fixing requires: polishing, clarifying, adding details
- Examples: trim Introduction, add effect sizes, fix grammar

MAJOR REVISION (2-3 months):
- Fixing requires: reframing, reanalyzing, major rewriting
- Examples: change research question, add control group, reframe gap as conceptual

If in doubt → Major Revision (better to overestimate time needed)"
```

---

## Instructor Facilitation Guide

### Pre-Class Preparation (30 min before workshop):

1. **Assign review pairs** (round-robin or topic-matched)
   ```
   Student A → reviews Papers B, C
   Student B → reviews Papers C, D
   Student C → reviews Papers D, E
   [etc., ensuring each paper gets ~2 reviews]
   ```

2. **Share papers** via Google Drive (anonymized IDs, not names)
   ```
   Paper_01.docx (Author: Anonymous_01)
   Paper_02.docx (Author: Anonymous_02)
   [Students only see ID, not real names]
   ```

3. **Calibration exercise** (optional, +10 min):
   - Provide one example paper
   - Class scores it together
   - Discuss scoring rationales
   - This ensures everyone is calibrated to same standards

### During Workshop (15 min):

**Minutes 1-2**: Quick instruction
> "You have 15 minutes to review 2 papers. Use the Top 5% rubric on the left. Focus on:
> - 5-dimension scoring (Novelty, Rigor, Clarity, Impact, Presentation)
> - 1 greatest strength
> - 3 prioritized weaknesses
> - 3-5 concrete suggestions with before→after examples
> Remember: Be constructive, not destructive. Give feedback you'd want to receive."

**Minutes 3-10**: Monitor & assist
- Watch students' Review Zones in Figma
- Intervene if:
  - Student stuck on scoring ("Is this 3 or 4?" → Ask "Does it meet top-tier standards?")
  - Vague feedback ("Improve clarity" → Ask "Can you quote a specific sentence?")
  - Too harsh/lenient (all 1s or all 5s → Remind of scoring anchors)

**Minutes 11-14**: Quality check
- Scan completed reviews
- Highlight exemplar reviews with @mentions
- React with 👍 to well-calibrated reviews
- Comment on reviews that need more specificity

**Minute 15**: Reflection & Share
- Students complete Meta-Reflection
- Quick poll: "Most common problem you saw?" (Show of hands)
- Instructor synthesis: "80% of papers had hedging issues, 60% had nominalization"

### Post-Workshop:

1. **Return reviews to authors** (anonymized)
   - "You received feedback from Peer_Reviewer_A and Peer_Reviewer_B"
   - Authors never know who reviewed them (reduces bias)

2. **Evaluate reviewer quality** (optional, for grading):
   - Did reviewer provide specific, actionable feedback?
   - Were scores calibrated (not all 1s or all 5s)?
   - Were suggestions constructive and evidence-based?

3. **Synthesize class patterns** for next lecture:
   - "Class-wide, we struggle with X - let's address that next week"

---

## Pedagogical Rationale

### Why Top 5% Standards?

**Aspirational Benchmarks**: Students should aim for Nature/Science level, even if targeting mid-tier journals. Why?
- **Ceiling Effect**: If you aim for "good enough for mid-tier," you might fall short
- **Rejection Cascade**: Papers often get rejected → resubmitted to lower-tier journals → aiming high initially saves time
- **Skill Transfer**: Learning top-tier standards improves writing even for lower-tier submissions

### Why Structured Rubric?

**Calibration**: Without explicit criteria, students vary wildly in harshness (some give all 5s, others all 1s)

**Objectivity**: Rubric reduces personal bias ("I don't like this topic" ≠ low Novelty score)

**Learning**: Students internalize top-tier criteria by applying them to others' work → improves own writing

### Why 2 Reviews Per Student?

**Reliability**: Single reviewer might be outlier (too harsh/lenient) - 2 reviewers provide balance

**Efficiency**: 2 reviews × 15 min = 30 min total workload (manageable), but provides rich feedback to author

**Reciprocity**: In class of 10 students, each doing 2 reviews → everyone gets ~2 reviews (round-robin logic)

### Learning Outcomes:

1. **Evaluative Judgment**: Ability to assess quality using explicit criteria
2. **Metacognition**: Recognize own mistakes by seeing them in others' writing
3. **Peer Learning**: Discover effective strategies by reading strong papers
4. **Constructive Communication**: Give criticism that helps, not just criticizes
5. **Editorial Thinking**: Understand how journal editors make accept/reject decisions

---

## Assessment Rubric (for Instructors Evaluating Peer Reviews)

| Criterion | Excellent (5) | Good (4) | Acceptable (3) | Weak (2) | Poor (1) |
|-----------|---------------|----------|----------------|----------|----------|
| **Calibration** | Scores match paper quality (not all 1s or 5s) | Mostly calibrated, minor inconsistencies | Some scores off | Many scores off | All 1s or all 5s |
| **Specificity** | All feedback with quotes + before→after | Most feedback specific | Some specific | Mostly vague | All generic |
| **Constructiveness** | Balanced strengths + weaknesses | Mostly constructive | Some negativity | Harsh tone | Destructive |
| **Actionability** | All suggestions implementable | Most implementable | Some vague | Mostly not actionable | No clear actions |
| **Depth** | Engages deeply with content | Good engagement | Surface-level | Minimal effort | Did not read paper |

**Target**: Students score 4-5 (Good to Excellent), demonstrating thoughtful, calibrated peer review skills.

---

## Example: Excellent Peer Review

```
🎯 NOVELTY: 5/5 (Conceptual gap)
Gap quality: Conceptual - proposes "Selective Consolidation Hypothesis"

Strengths:
• Novel theoretical framework (utility-based, not just strength-based consolidation)
• Challenges dominant theory (Synaptic Homeostasis Hypothesis)

Concerns:
• None - this is a clear conceptual contribution

───────────────────────────────────────────────────────────────────

🔬 RIGOR: 5/5
Reproducibility: 30/30 (all 6 elements perfect)
Control: Excellent (sham TMR + no-TMR groups)
Statistics: Strong (power=.90, pre-registered, all assumptions checked)

Strengths:
• Pre-registered RCT with OSF materials
• Two control conditions (sham + no-TMR)
• Comprehensive robustness checks

Concerns:
• None - this is gold-standard rigor

───────────────────────────────────────────────────────────────────

💎 CLARITY: 3/5
Main message: "Memory consolidation is selective based on utility, not strength"
Logical flow: Smooth, but Discussion is too long (8 paragraphs)
Figures: Excellent (Figure 2 shows selective consolidation beautifully)

Strengths:
• Main message crystal clear
• Beautiful Figure 2 (self-explanatory)

Concerns:
• Discussion is 8 paragraphs - should be ~5
• Some nominalization ("investigation of" → "investigated")

───────────────────────────────────────────────────────────────────

🌍 IMPACT: 5/5
Broader significance: High (memory + decision-making + sleep fields)
Citation potential: High (will be cited by multiple subfields)

Strengths:
• Cross-disciplinary appeal
• Strong translational angle (sleep optimization for learning)

Concerns:
• None - clear high-impact work

───────────────────────────────────────────────────────────────────

✨ PRESENTATION: 4/5
Error count: 2 (one comma splice in Discussion p. 6, one missing DOI in Ref #12)
APA: Excellent otherwise

Strengths:
• Professional quality throughout
• Beautiful figures

Concerns:
• Fix comma splice: "Results were significant, however" → "significant; however,"
• Add DOI for Smith et al. (2019) reference

───────────────────────────────────────────────────────────────────

🎯 WEIGHTED TOTAL: 4.5/5
(5×0.2) + (5×0.3) + (3×0.2) + (5×0.2) + (4×0.1) = 4.5/5

📈 OVERALL: TOP 5% - Nature Neuroscience level

───────────────────────────────────────────────────────────────────

💪 GREATEST STRENGTH: Novel Conceptual Framework
The "Selective Consolidation Hypothesis" is a genuinely new way of thinking about
memory consolidation - utility-based rather than strength-based. This challenges
dominant theories and will likely generate follow-up work.

───────────────────────────────────────────────────────────────────

⚠️ TOP 3 WEAKNESSES:

1. [LOW PRIORITY] Discussion too long (8 paragraphs)
   Section: Discussion
   Why this matters: Readers will lose the main message
   Suggested fix: Merge paragraphs 4-5 (both about limitations), delete paragraph 7
   (speculative future directions). Target: 5 paragraphs.

2. [LOW PRIORITY] Minor nominalization issues
   Section: Introduction p. 3, Discussion p. 2
   Why this matters: Reduces clarity and adds words
   Suggested fix:
   BEFORE: "Investigation of selective consolidation revealed..."
   AFTER: "We investigated selective consolidation and found..."

3. [VERY LOW] Presentation errors (2 total)
   Section: Discussion p. 6, References #12
   Why this matters: Top journals desk-reject for careless errors
   Suggested fix:
   - Comma splice: "significant, however" → "significant; however,"
   - Add DOI: Smith et al. (2019) missing DOI

───────────────────────────────────────────────────────────────────

💡 CONCRETE SUGGESTIONS:

1. [Discussion] Trim from 8 to 5 paragraphs
   - Merge paragraphs 4-5 (limitations)
   - Delete paragraph 7 (too speculative)
   - Keep paragraphs 1 (summary), 2 (interpretation), 3 (alternative explanations),
     4-5 merged (limitations), 6 (conclusion)

2. [Introduction p. 3] Fix nominalization
   BEFORE: "Investigation of selective consolidation has revealed..."
   AFTER: "Studies investigating selective consolidation have revealed..."

3. [Discussion p. 6] Fix comma splice
   BEFORE: "Results were significant, however the effect was small"
   AFTER: "Results were significant; however, the effect was small"

4. [References #12] Add missing DOI
   Smith, J., et al. (2019). Title. Journal, 10, 123-145.
   [Add DOI: https://doi.org/10.xxxx/xxxxx]

───────────────────────────────────────────────────────────────────

✅ EDITORIAL DECISION: ACCEPT (Minor Revision)

RATIONALE:
This is exceptional work - novel conceptual framework, flawless methods, high
impact. The issues are trivial (Discussion length, minor writing, 2 errors) and
fixable in 1-2 days. This is Nature Neuroscience quality. I would accept pending
minor revisions.

───────────────────────────────────────────────────────────────────

🤝 TONE CHECK:
✓ Balanced strengths + weaknesses
✓ All feedback specific with examples
✓ All suggestions actionable
✓ Explained why each issue matters
✓ Wrote feedback I'd want to receive
```

**Why This Is Excellent**:
- ✅ Calibrated scores (not all 5s, recognizes Clarity is 3/5 due to Discussion length)
- ✅ Specific feedback (quotes exact text, provides before→after fixes)
- ✅ Prioritized weaknesses (low/very low, acknowledging minor nature)
- ✅ Constructive tone (leads with strengths, explains rationale)
- ✅ Actionable (author knows exactly what to do: trim Discussion, fix 2 sentences, add DOI)
