# Example 4.2: Good Methods Section (Complete Reproducibility)

**Purpose**: Demonstrate Nature Human Behaviour-level Methods with all 6 checklist elements complete

**Use in Workshop**: Model for Template 4.3 (Reproducibility Checklist) and comparison with Example 4.1

---

## Research Topic

**Domain**: Social Psychology - Emotion Regulation and Well-Being
**Study Type**: Experimental intervention (cognitive reappraisal training)
**Same study as Example 4.1, but with all reproducibility gaps fixed**

---

## Good Methods Section (Bulletproofed)

```
Participants

We recruited 85 undergraduate students (18-25 years old, native English
speakers, no current psychotherapy or psychiatric medication) from
Introductory Psychology courses at [University Name] via course announcements
and the SONA participant pool (October-November 2024). Exclusion criteria
included history of severe mental illness (schizophrenia, bipolar disorder)
per self-report screening.

Five participants were excluded: 3 failed attention checks (<80% accuracy
on instructional manipulation checks; Oppenheimer et al., 2009), 1 did not
complete the post-assessment, and 1 withdrew consent during the intervention.
Final sample: N=80 (40 per condition), which provided 80% power to detect
medium effect sizes (d=0.63) at α=.05 (G*Power 3.1.9.7; Faul et al., 2007).

The final sample comprised 52 female and 28 male participants (M_age = 19.2
years, SD = 1.4, range 18-24). Racial/ethnic composition: 45% White, 25%
Asian, 15% Hispanic/Latino, 15% Other/Multiracial. All participants were
first- or second-year undergraduates. Participants received course credit
(0.5 credits). All procedures were approved by the [University] Institutional
Review Board (Protocol #2024-PSY-1234) and followed APA ethical guidelines.

Materials

Emotion Regulation Questionnaire (ERQ; Gross & John, 2003). The ERQ is a
10-item self-report measure assessing two emotion regulation strategies:
Cognitive Reappraisal (6 items; e.g., "When I want to feel less negative
emotion, I change what I'm thinking about") and Expressive Suppression
(4 items; e.g., "I keep my emotions to myself"). Participants responded on
a 7-point Likert scale (1 = strongly disagree, 7 = strongly agree). Scale
scores were calculated by averaging items (Reappraisal: range 1-7;
Suppression: range 1-7). Higher scores indicate greater use of each strategy.

Cronbach's α in the current sample: Reappraisal = .82, Suppression = .76,
consistent with normative values (Reappraisal α = .79, Suppression α = .73;
Gross & John, 2003). The ERQ has demonstrated convergent validity with other
emotion regulation measures (r = .40-.65; John & Gross, 2004) and test-retest
reliability over 3 months (r = .69; Gross & John, 2003). Full scale available
at https://osf.io/xyz123/.

Satisfaction With Life Scale (SWLS; Diener et al., 1985). The SWLS is a
5-item measure of global life satisfaction (e.g., "In most ways my life is
close to my ideal"). Participants responded on a 7-point Likert scale
(1 = strongly disagree, 7 = strongly agree). Total scores were calculated
by summing all items (range: 5-35), with higher scores indicating greater
life satisfaction. The SWLS has strong internal consistency (α = .87;
Diener et al., 1985), 2-month test-retest reliability (r = .82; Diener et al.,
1985), and convergent validity with other well-being measures (r = .50-.75;
Pavot & Diener, 1993).

Cronbach's α in the current sample = .88, consistent with normative α = .87
(Diener et al., 1985). Sample item: "In most ways my life is close to my
ideal." Full scale available in Diener et al. (1985) and at
https://osf.io/abc456/.

Cognitive Reappraisal Training Protocol. We developed a 60-minute single-session
intervention based on established cognitive reappraisal protocols (Jamieson et al.,
2012; McRae et al., 2012). The intervention was delivered via pre-recorded video
presentation (standardized across participants to control for instructor effects)
accessible through Qualtrics (Version December 2024). Training materials and video
available at https://osf.io/def789/.

Training structure: (1) Psychoeducation on emotion regulation and reappraisal
(10 minutes; PowerPoint slides with narration); (2) Cognitive reappraisal techniques
demonstration (20 minutes; video examples of reframing negative events, perspective-
taking, distancing; based on Gross, 2002); (3) Guided practice with 5 standardized
scenarios (20 minutes; participants typed reappraisal responses, received automated
feedback based on keyword matching; adapted from Denny & Ochsner, 2014);
(4) Homework planning (10 minutes; participants created daily reappraisal diary
template). Training manual with all scenarios and instructions available at
https://osf.io/ghi012/.

Active Control Condition. To control for time, attention, and expectancy effects,
the control group viewed a 60-minute health documentary ("The Human Body: Systems,"
BBC, 2018) delivered via the same Qualtrics platform. Documentary content was
matched for engagement and audiovisual complexity but contained no emotion regulation
content (verified by two independent raters; κ = .89). Control participants also
completed daily logs (parallel structure to training group homework) but received
no review or feedback. Control materials available at https://osf.io/jkl345/.

Procedure

All procedures were conducted online via Qualtrics (Version December 2024, Qualtrics
XM; Provo, UT). Participants were recruited via SONA and randomly assigned to
conditions using Qualtrics' automated randomization function (simple randomization,
no stratification, allocation concealment maintained until participant clicked
study link).

Session 1 (Baseline Assessment, Day 0). After providing informed consent via
electronic signature, participants completed demographic questions (age, gender,
race/ethnicity, year in school), followed by baseline ERQ and SWLS (presented in
randomized order using Qualtrics randomization to control for order effects).
Three instructional manipulation checks (IMCs) were embedded within questionnaires
(e.g., "To show you are reading carefully, please select 'Strongly Agree' for
this item"; Oppenheimer et al., 2009). Participants scoring <80% accuracy on IMCs
were excluded from analysis. Session duration: M = 12 minutes (SD = 3).

Session 2 (Intervention, Days 1-3). Within 1-3 days of baseline (M = 1.8 days,
SD = 0.9), participants received an email with a link to the intervention session
(randomized assignment revealed at this point). Participants were instructed to
complete the session in one sitting in a quiet environment with headphones.
Intervention: Training group watched the 60-minute reappraisal training video and
completed practice exercises; control group watched the 60-minute documentary.
Completion was verified via Qualtrics timestamps. Both groups received identical
homework instructions (daily practice logs for 14 days) delivered via automated
email reminders (Days 4, 7, 10, 13). Session duration: M = 64 minutes (SD = 8)
for training, M = 62 minutes (SD = 6) for control; no significant difference,
t(78) = 1.2, p = .23.

Session 3 (Post-Assessment, Day 14 ± 1). Exactly 14 days after the intervention
session (±1 day to accommodate participant availability), participants received
an email link to the post-assessment. Participants completed the same measures
as baseline (ERQ, SWLS) in randomized order. Automated email reminders were sent
at Days 12 and 13 for non-responders. Post-assessment completion: 78/80 participants
(97.5%) completed on Day 14, 2 participants completed on Day 15. Session duration:
M = 10 minutes (SD = 2).

All sessions were self-paced and could be paused and resumed (timestamps recorded).
Participants could withdraw at any time without penalty. Upon completion, participants
were debriefed via electronic form explaining the study purpose and providing mental
health resources.

Data Processing and Analysis

All analyses were conducted using R version 4.2.1 (R Core Team, 2022) with RStudio
2022.07.1. Analysis code, raw data (with participant identifiers removed), and
materials are available at https://osf.io/mno678/.

Data Preprocessing. Data were downloaded from Qualtrics as CSV files and imported
to R using the readr package (version 2.1.2). Missing item-level data were minimal
(0.3% of all items). We used listwise deletion for the 2 participants who did not
complete post-assessment, resulting in final N = 80 for all analyses. No other
participants had missing data. Scale scores were calculated by averaging items
(ERQ) or summing items (SWLS) according to published scoring procedures.

Outlier Treatment. We identified outliers as values >3 SD from the group mean for
each measure at each timepoint. Three outliers were identified (3.8% of all data
points: 2 on ERQ-Reappraisal at baseline, 1 on SWLS at post-assessment). Following
recommendations for small samples (Leys et al., 2013), outliers were winsorized to
3 SD from the mean rather than excluded. Sensitivity analyses (reported in
Supplementary Materials) showed results were unchanged when outliers were excluded
entirely.

Primary Analysis. We tested the hypothesis that the training group would show
greater increases in reappraisal use and life satisfaction from baseline to
post-assessment compared to the control group using 2 (Group: Training vs Control)
× 2 (Time: Baseline vs Post-Assessment) mixed-design ANOVAs for each outcome
(ERQ-Reappraisal, SWLS). Group was the between-subjects factor, Time was the
within-subjects factor.

We conducted ANOVAs using the aov() function from the stats package in R (R Core
Team, 2022). For the repeated-measures factor (Time), we tested the sphericity
assumption using Mauchly's test (Mauchly, 1940). If sphericity was violated
(p < .05), we applied the Greenhouse-Geisser correction to degrees of freedom
(Greenhouse & Geisser, 1959). For between-subjects effects, we tested homogeneity
of variance using Levene's test (Levene, 1960) and normality using Shapiro-Wilk
tests (Shapiro & Wilk, 1965) for each group at each timepoint.

Assumption Testing Results. Shapiro-Wilk tests indicated no violations of normality
for any variable in either group at either timepoint (all ps > .05). Levene's tests
indicated no violations of homogeneity of variance for any variable (ERQ-Reappraisal:
F(1,78) = 1.45, p = .23; SWLS: F(1,78) = 0.89, p = .35). Mauchly's test indicated
sphericity was not violated for the Time factor (all ps > .05), so no corrections
were necessary.

Effect sizes were calculated as partial eta-squared (η²_p) for omnibus tests and
Cohen's d for pairwise comparisons using the effectsize package (version 0.8.0;
Ben-Shachar et al., 2020). We used conventional thresholds: η²_p = .01 (small),
.06 (medium), .14 (large); Cohen's d = 0.2 (small), 0.5 (medium), 0.8 (large)
(Cohen, 1988).

Statistical significance was set at α = .05 (two-tailed) for all tests. We did
not adjust for multiple comparisons across the two primary outcomes (ERQ-Reappraisal,
SWLS) because they tested distinct a priori hypotheses (Rothman, 1990), but we
reported exact p-values to allow readers to evaluate evidence strength.

Secondary Analyses. To test whether increased reappraisal use mediated the effect
of training on life satisfaction, we conducted a mediation analysis using the
mediation package (version 4.5.0; Tingley et al., 2014). We regressed change scores
(post-baseline) for SWLS on change scores for ERQ-Reappraisal and Group (Training
vs Control coded as 1 vs 0), testing the indirect effect of Group on SWLS change
through ERQ-Reappraisal change. We estimated 95% confidence intervals using
bias-corrected bootstrap with 5,000 iterations. An indirect effect was considered
significant if the 95% CI excluded zero.

Power Analysis. Sample size was determined a priori using G*Power 3.1.9.7 (Faul
et al., 2007). For a 2×2 mixed ANOVA with α = .05, power = .80, and a medium
effect size for the Group × Time interaction (f = 0.25, equivalent to η²_p = .06,
based on previous reappraisal training studies; Jamieson et al., 2012; McRae et al.,
2012), we required N = 34 per group. We oversampled to N = 42-43 per group to
account for anticipated 10% attrition, ultimately achieving N = 40 per group after
exclusions.
```

---

## Reproducibility Checklist Score: 30/30 (100%) ✅ EXCELLENT

### Element-by-Element Analysis

#### ELEMENT 1: PARTICIPANTS - 5/5 ✅ COMPLETE

**What's PRESENT**:
- ✅ Recruitment method: SONA participant pool, Intro Psych courses (lines 4-7)
- ✅ Recruitment timing: October-November 2024 (line 7)
- ✅ Inclusion criteria: 18-25 years, native English, no current treatment (lines 3-5)
- ✅ Exclusion criteria: Severe mental illness history (lines 7-9)
- ✅ Excluded N + reasons: 5 excluded (3 IMC failures, 1 incomplete, 1 withdrawal) (lines 11-14)
- ✅ Final N + per group: N=80, 40 per condition (line 14)
- ✅ Power analysis: 80% power for d=0.63, N=34 required (lines 14-16)
- ✅ Demographics: M age=19.2, SD=1.4, range 18-24; 52F/28M (lines 18-20)
- ✅ Race/ethnicity: 45% White, 25% Asian, 15% Hispanic, 15% Other (line 20)
- ✅ Education level: First- or second-year undergraduates (lines 21-22)
- ✅ Compensation: 0.5 course credits (line 22)
- ✅ IRB approval: Protocol #2024-PSY-1234, APA guidelines (lines 22-24)

**Why 5/5**: All 12 sub-items present with specific details

---

#### ELEMENT 2: MATERIALS - 5/5 ✅ COMPLETE

**What's PRESENT**:

**ERQ (lines 26-44)**:
- ✅ Scale name + citation: Emotion Regulation Questionnaire (Gross & John, 2003)
- ✅ Number of items: 10 items (2 subscales: 6 + 4)
- ✅ Response format: 7-point Likert (1=strongly disagree, 7=strongly agree)
- ✅ Scoring procedure: Average items per subscale, range 1-7
- ✅ Sample items: "When I want to feel less negative emotion..." (Reappraisal), "I keep my emotions to myself" (Suppression)
- ✅ Reliability (current sample): α = .82 (Reappraisal), .76 (Suppression)
- ✅ Reliability (normative): α = .79, .73 (Gross & John, 2003)
- ✅ Validity evidence: Convergent r=.40-.65 (John & Gross, 2004), test-retest r=.69
- ✅ Availability: https://osf.io/xyz123/

**SWLS (lines 46-61)**:
- ✅ Scale name + citation: Satisfaction With Life Scale (Diener et al., 1985)
- ✅ Number of items: 5 items
- ✅ Response format: 7-point Likert (1=strongly disagree, 7=strongly agree)
- ✅ Scoring procedure: Sum all items, range 5-35
- ✅ Sample item: "In most ways my life is close to my ideal"
- ✅ Reliability (current): α = .88
- ✅ Reliability (normative): α = .87 (Diener et al., 1985)
- ✅ Validity evidence: Convergent r=.50-.75 (Pavot & Diener, 1993), test-retest r=.82
- ✅ Availability: Diener et al. (1985) + https://osf.io/abc456/

**Training Protocol (lines 63-81)**:
- ✅ Protocol basis: Jamieson et al., 2012; McRae et al., 2012
- ✅ Duration: 60-minute single session
- ✅ Delivery method: Pre-recorded video via Qualtrics (December 2024 version)
- ✅ Content structure: (1) Psychoeducation 10min, (2) Techniques demo 20min, (3) Practice 20min, (4) Homework 10min
- ✅ Specific techniques: Reframing, perspective-taking, distancing (Gross, 2002)
- ✅ Practice details: 5 standardized scenarios, typed responses, automated keyword feedback (Denny & Ochsner, 2014)
- ✅ Availability: Training manual + video at https://osf.io/def789/, scenarios at https://osf.io/ghi012/

**Control Condition (lines 83-91)**:
- ✅ Control type: Active control (time + attention matched)
- ✅ Content: "The Human Body: Systems" documentary (BBC, 2018)
- ✅ Duration: 60 minutes (matched to training)
- ✅ Delivery: Same Qualtrics platform
- ✅ Content verification: Two independent raters confirmed no emotion regulation content (κ=.89)
- ✅ Homework: Daily logs (parallel structure, no feedback)
- ✅ Availability: Control materials at https://osf.io/jkl345/

**Why 5/5**: All 4 materials fully specified with names, versions, psychometrics, sample items, availability

---

#### ELEMENT 3: PROCEDURE - 5/5 ✅ COMPLETE

**What's PRESENT**:

**Overall** (lines 93-96):
- ✅ Platform: Qualtrics (Version December 2024)
- ✅ Randomization: Qualtrics automated function, simple randomization, allocation concealment

**Session 1 - Baseline** (lines 98-107):
- ✅ Timing: Day 0
- ✅ Consent: Electronic signature
- ✅ Measures: Demographics → ERQ → SWLS (randomized order)
- ✅ Quality control: 3 instructional manipulation checks (Oppenheimer et al., 2009), <80% accuracy = exclusion
- ✅ Duration: M=12 min, SD=3

**Session 2 - Intervention** (lines 109-123):
- ✅ Timing: Days 1-3 (M=1.8, SD=0.9)
- ✅ Delivery: Email link, complete in one sitting, quiet environment, headphones
- ✅ Training group: 60-min video + practice (M=64 min, SD=8)
- ✅ Control group: 60-min documentary (M=62 min, SD=6)
- ✅ Verification: Qualtrics timestamps, no group difference t(78)=1.2, p=.23
- ✅ Homework: Daily logs for 14 days, email reminders Days 4/7/10/13

**Session 3 - Post-Assessment** (lines 125-131):
- ✅ Timing: Day 14 ± 1 (exact scheduling)
- ✅ Delivery: Email link with reminders at Days 12/13
- ✅ Measures: Same as baseline (ERQ, SWLS, randomized order)
- ✅ Completion rate: 78/80 (97.5%) on Day 14, 2 on Day 15
- ✅ Duration: M=10 min, SD=2

**Participant Experience** (lines 133-137):
- ✅ Flexibility: Self-paced, pause/resume allowed (timestamps recorded)
- ✅ Withdrawal: Could withdraw anytime without penalty
- ✅ Debriefing: Electronic form with study purpose + mental health resources

**Why 5/5**: Complete chronological protocol with timing, instructions, environment, quality controls

---

#### ELEMENT 4: PARAMETERS - 5/5 ✅ COMPLETE

**What's PRESENT**:
- ✅ Training duration: 60 minutes (exact)
- ✅ Training components: 10min + 20min + 20min + 10min (breakdown)
- ✅ Practice scenarios: 5 standardized scenarios (exact number)
- ✅ Documentary duration: 60 minutes (matched)
- ✅ Homework period: 14 days (exact)
- ✅ Reminder schedule: Days 4, 7, 10, 13 (specific days)
- ✅ Post-assessment timing: Day 14 ± 1 (exact interval + tolerance)
- ✅ Baseline to intervention: Days 1-3 (M=1.8, SD=0.9) (interval + variability)
- ✅ Session completion rate: 97.5% on Day 14 (compliance metric)
- ✅ IMC threshold: <80% accuracy = exclusion (decision rule)
- ✅ Outlier threshold: >3 SD from group mean (detection rule)
- ✅ Alpha level: .05, two-tailed (statistical threshold)
- ✅ Power: 80%, effect size d=0.63 (design parameters)
- ✅ Bootstrap iterations: 5,000 (mediation analysis)

**Why 5/5**: All numeric values, units, ranges, thresholds specified

---

#### ELEMENT 5: SOFTWARE & EQUIPMENT - 5/5 ✅ COMPLETE

**What's PRESENT**:
- ✅ Survey platform: Qualtrics Version December 2024 (Qualtrics XM, Provo, UT)
- ✅ Analysis software: R version 4.2.1 (R Core Team, 2022)
- ✅ IDE: RStudio 2022.07.1
- ✅ Packages with versions:
  - readr 2.1.2 (data import)
  - stats (base R for ANOVA, aov() function)
  - effectsize 0.8.0 (effect size calculation; Ben-Shachar et al., 2020)
  - mediation 4.5.0 (mediation analysis; Tingley et al., 2014)
- ✅ Power analysis software: G*Power 3.1.9.7 (Faul et al., 2007)
- ✅ Randomization method: Qualtrics automated randomization (simple, no stratification)
- ✅ Code availability: https://osf.io/mno678/ (with data and materials)

**Why 5/5**: All software names, versions, specific functions, settings documented

---

#### ELEMENT 6: DATA PROCESSING & ANALYSIS - 5/5 ✅ COMPLETE

**What's PRESENT**:

**Preprocessing** (lines 142-148):
- ✅ Data format: CSV from Qualtrics, imported via readr 2.1.2
- ✅ Missing data: 0.3% item-level, listwise deletion for 2 incomplete post-assessments
- ✅ Final N: 80 (after exclusions)
- ✅ Scoring: Average (ERQ) or sum (SWLS) per published procedures

**Outlier Handling** (lines 150-157):
- ✅ Detection rule: >3 SD from group mean per timepoint
- ✅ Outliers identified: 3 (3.8% of data: 2 ERQ-R baseline, 1 SWLS post)
- ✅ Treatment: Winsorized to 3 SD (Leys et al., 2013)
- ✅ Sensitivity analysis: Results unchanged with exclusion (in Supplement)

**Primary Analysis** (lines 159-169):
- ✅ Statistical test: 2×2 mixed ANOVA (Group × Time)
- ✅ Function: aov() from stats package (R Core Team, 2022)
- ✅ Factors: Group (between), Time (within)
- ✅ Outcomes: ERQ-Reappraisal, SWLS (separate ANOVAs)

**Assumptions Testing** (lines 171-179):
- ✅ Sphericity: Mauchly's test (Mauchly, 1940), correction if p<.05 (Greenhouse-Geisser, 1959)
- ✅ Homogeneity: Levene's test (Levene, 1960)
- ✅ Normality: Shapiro-Wilk tests (Shapiro & Wilk, 1965) per group per timepoint
- ✅ Results: All assumptions met (normality ps>.05, Levene's ERQ F(1,78)=1.45 p=.23, SWLS F(1,78)=0.89 p=.35, sphericity ps>.05)

**Effect Sizes** (lines 181-184):
- ✅ Measures: Partial η²_p (omnibus), Cohen's d (pairwise)
- ✅ Package: effectsize 0.8.0 (Ben-Shachar et al., 2020)
- ✅ Thresholds: η²_p .01/.06/.14 (small/med/large), d 0.2/0.5/0.8 (Cohen, 1988)

**Statistical Inference** (lines 186-190):
- ✅ Alpha: .05, two-tailed
- ✅ Multiple comparisons: No adjustment (distinct a priori hypotheses; Rothman, 1990)
- ✅ Reporting: Exact p-values for evidence evaluation

**Secondary Analyses** (lines 192-199):
- ✅ Mediation analysis: mediation package 4.5.0 (Tingley et al., 2014)
- ✅ Model: Group → ERQ-Reappraisal change → SWLS change
- ✅ Inference: 95% CI via bias-corrected bootstrap, 5,000 iterations
- ✅ Criterion: Significant if 95% CI excludes zero

**Power Analysis** (lines 201-208):
- ✅ Software: G*Power 3.1.9.7 (Faul et al., 2007)
- ✅ Design: 2×2 mixed ANOVA
- ✅ Parameters: α=.05, power=.80, f=0.25 (η²_p=.06)
- ✅ Basis: Previous studies (Jamieson et al., 2012; McRae et al., 2012)
- ✅ Required N: 34 per group
- ✅ Actual recruitment: N=42-43 per group (10% attrition buffer)
- ✅ Final N: 40 per group after exclusions

**Why 5/5**: Complete transparency on all preprocessing, exclusions, assumptions, corrections, justifications

---

## Comparison Table: Bad (Example 4.1) vs Good (Example 4.2)

| Element | Bad Methods (4.1) Score | Good Methods (4.2) Score | Key Improvements |
|---------|-------------------------|--------------------------|------------------|
| **1. Participants** | 1/5 ⚠️ INSUFFICIENT | 5/5 ✅ COMPLETE | Added: Recruitment method (SONA pool, Intro Psych courses, Oct-Nov 2024), inclusion/exclusion criteria (18-25yo, English, no treatment; no severe MI), excluded N with reasons (5: 3 IMC, 1 incomplete, 1 withdrew), demographics (M=19.2, 52F/28M, race/ethnicity), power analysis (80% for d=.63, N=34 required), IRB approval (#2024-PSY-1234) |
| **2. Materials** | 0/5 ❌ MISSING | 5/5 ✅ COMPLETE | **ERQ**: Added full citation (Gross & John, 2003), 10 items (6 Reappraisal + 4 Suppression), 7-point Likert, scoring (average, 1-7 range), sample items for both subscales, reliability (current α=.82/.76, normative α=.79/.73), validity (convergent r=.40-.65, test-retest r=.69), OSF link. **SWLS**: Added full citation (Diener et al., 1985), 5 items, 7-point Likert, scoring (sum, 5-35 range), sample item, reliability (current α=.88, normative α=.87), validity (convergent r=.50-.75, test-retest r=.82), availability. **Training**: Protocol basis (Jamieson et al., 2012; McRae et al., 2012), 60-min structure (10+20+20+10), delivery (pre-recorded video, Qualtrics Dec 2024), content (psychoeducation, techniques, 5 practice scenarios with automated feedback), OSF materials. **Control**: Active control (60-min BBC documentary), content verification (2 raters, κ=.89), matched time/attention, parallel homework |
| **3. Procedure** | 1/5 ⚠️ INSUFFICIENT | 5/5 ✅ COMPLETE | Added: Platform (Qualtrics Dec 2024), randomization (automated, allocation concealed). **Session 1 (Day 0)**: E-consent, demographics → ERQ/SWLS (randomized order), 3 IMCs (<80%=exclude), M=12min. **Session 2 (Days 1-3, M=1.8)**: Email link, one sitting, quiet+headphones, training (M=64min, SD=8) vs control (M=62min, SD=6, ns difference), timestamps, 14-day homework with reminders (Days 4/7/10/13). **Session 3 (Day 14±1)**: Same measures (randomized), reminders Days 12/13, 97.5% on Day 14, M=10min. Self-paced, pause/resume allowed, withdraw anytime, electronic debriefing |
| **4. Parameters** | 0/5 ❌ MISSING | 5/5 ✅ COMPLETE | Added: Training duration (60min exact: 10+20+20+10), practice scenarios (5 standardized), documentary (60min matched), homework (14 days, reminders Days 4/7/10/13), post-assessment (Day 14±1), baseline-intervention interval (Days 1-3, M=1.8, SD=0.9), compliance (97.5% Day 14), IMC threshold (<80%=exclude), outlier rule (>3 SD winsorized), alpha (.05 two-tailed), power (80%, d=.63), bootstrap iterations (5,000) |
| **5. Software** | 0/5 ❌ MISSING | 5/5 ✅ COMPLETE | Added: Qualtrics (Version Dec 2024, Qualtrics XM, Provo UT), R 4.2.1 (R Core Team, 2022), RStudio 2022.07.1, packages (readr 2.1.2, stats aov(), effectsize 0.8.0, mediation 4.5.0), G*Power 3.1.9.7 (Faul et al., 2007), randomization (Qualtrics automated simple randomization), code/data availability (OSF link) |
| **6. Analysis** | 2/5 ⚠️ WEAK | 5/5 ✅ COMPLETE | **Preprocessing**: CSV import (readr 2.1.2), 0.3% missing items, listwise deletion (2 incomplete), scoring per published procedures. **Outliers**: Detection (>3 SD), treatment (winsorized, Leys et al., 2013), sensitivity analysis (results unchanged). **Primary**: 2×2 mixed ANOVA (Group×Time) via aov() for ERQ-R and SWLS. **Assumptions**: Mauchly sphericity (Greenhouse-Geisser if p<.05), Levene homogeneity (ERQ F=1.45 p=.23, SWLS F=0.89 p=.35), Shapiro-Wilk normality (all ps>.05). **Effect sizes**: η²_p + d (effectsize 0.8.0), thresholds (.01/.06/.14, 0.2/0.5/0.8). **Inference**: α=.05 two-tailed, no MC adjustment (distinct hypotheses, Rothman 1990), exact ps reported. **Mediation**: mediation 4.5.0, Group→ERQ-R→SWLS, 5,000 bootstrap, 95% CI. **Power**: G*Power 3.1.9.7, f=.25 (previous studies), N=34 required, recruited 42-43 (10% buffer), final N=40 |
| **TOTAL** | **6/30 (20%)** ❌ | **30/30 (100%)** ✅ | Transformed from INSUFFICIENT to EXCELLENT by addressing all 24 critical gaps identified in AI audit |

---

## Before → After Transformations (AI Audit Fixes Applied)

### Fix 1: Participant Recruitment

**❌ BEFORE** (Example 4.1):
```
Eighty undergraduate students participated in the study for course credit.
```

**✅ AFTER** (Example 4.2):
```
We recruited 85 undergraduate students (18-25 years old, native English
speakers, no current psychotherapy or psychiatric medication) from
Introductory Psychology courses at [University Name] via course
announcements and the SONA participant pool (October-November 2024).

Five participants were excluded: 3 failed attention checks (<80% accuracy),
1 did not complete the post-assessment, and 1 withdrew consent. Final
sample: N=80 (40 per condition).
```

**Improvement**: Adds recruitment source (SONA, Intro Psych courses), timing (Oct-Nov 2024), inclusion criteria (age, language, treatment status), excluded N with specific reasons, flow from recruited (85) → excluded (5) → final (80)

---

### Fix 2: Inclusion/Exclusion Criteria

**❌ BEFORE** (Example 4.1):
```
[Not mentioned - no screening criteria]
```

**✅ AFTER** (Example 4.2):
```
Inclusion criteria: 18-25 years old, native English speakers, no current
psychotherapy or psychiatric medication.

Exclusion criteria: History of severe mental illness (schizophrenia,
bipolar disorder) per self-report screening.
```

**Improvement**: Explicit inclusion/exclusion with operational definitions, verifiable via screening

---

### Fix 3: Demographics

**❌ BEFORE** (Example 4.1):
```
[Not mentioned - no demographic information]
```

**✅ AFTER** (Example 4.2):
```
The final sample comprised 52 female and 28 male participants (M_age = 19.2
years, SD = 1.4, range 18-24). Racial/ethnic composition: 45% White, 25%
Asian, 15% Hispanic/Latino, 15% Other/Multiracial. All participants were
first- or second-year undergraduates.
```

**Improvement**: Complete demographics (M/SD/range age, gender counts, race/ethnicity percentages, education level) allowing sample representativeness evaluation

---

### Fix 4: Power Analysis & Sample Size Justification

**❌ BEFORE** (Example 4.1):
```
[Not mentioned - N=80 stated without justification]
```

**✅ AFTER** (Example 4.2):
```
Final sample: N=80 (40 per condition), which provided 80% power to detect
medium effect sizes (d=0.63) at α=.05 (G*Power 3.1.9.7; Faul et al., 2007).

[Later in Methods:]
Sample size was determined a priori using G*Power 3.1.9.7 (Faul et al., 2007).
For a 2×2 mixed ANOVA with α = .05, power = .80, and a medium effect size
for the Group × Time interaction (f = 0.25, equivalent to η²_p = .06, based
on previous reappraisal training studies; Jamieson et al., 2012; McRae et al.,
2012), we required N = 34 per group. We oversampled to N = 42-43 per group
to account for anticipated 10% attrition.
```

**Improvement**: A priori power analysis with software (G*Power), design parameters (2×2 mixed ANOVA, α=.05, power=.80), expected effect (f=.25, η²_p=.06 from prior studies), required N (34/group), oversampling strategy (10% buffer → 42-43 recruited), final N (40/group after exclusions)

---

### Fix 5: Emotion Regulation Measure Specification

**❌ BEFORE** (Example 4.1):
```
Participants completed questionnaires measuring emotion regulation and well-being.
```

**✅ AFTER** (Example 4.2):
```
Emotion Regulation Questionnaire (ERQ; Gross & John, 2003). The ERQ is a
10-item self-report measure assessing two emotion regulation strategies:
Cognitive Reappraisal (6 items; e.g., "When I want to feel less negative
emotion, I change what I'm thinking about") and Expressive Suppression
(4 items; e.g., "I keep my emotions to myself"). Participants responded on
a 7-point Likert scale (1 = strongly disagree, 7 = strongly agree). Scale
scores were calculated by averaging items (Reappraisal: range 1-7;
Suppression: range 1-7). Higher scores indicate greater use of each strategy.

Cronbach's α in the current sample: Reappraisal = .82, Suppression = .76,
consistent with normative values (Reappraisal α = .79, Suppression α = .73;
Gross & John, 2003). The ERQ has demonstrated convergent validity with other
emotion regulation measures (r = .40-.65; John & Gross, 2004) and test-retest
reliability over 3 months (r = .69; Gross & John, 2003). Full scale available
at https://osf.io/xyz123/.
```

**Improvement**: From vague "questionnaires" to complete specification: Full citation (Gross & John, 2003), 10 items (6+4 subscales), 7-point Likert, scoring procedure (average, 1-7 range), directional interpretation, sample items for both subscales, current reliability (α=.82/.76), normative reliability (α=.79/.73), convergent validity (r=.40-.65), test-retest reliability (r=.69), OSF availability

---

### Fix 6: Well-Being Measure Specification

**❌ BEFORE** (Example 4.1):
```
[Same vague "questionnaires measuring well-being"]
```

**✅ AFTER** (Example 4.2):
```
Satisfaction With Life Scale (SWLS; Diener et al., 1985). The SWLS is a
5-item measure of global life satisfaction (e.g., "In most ways my life is
close to my ideal"). Participants responded on a 7-point Likert scale
(1 = strongly disagree, 7 = strongly agree). Total scores were calculated
by summing all items (range: 5-35), with higher scores indicating greater
life satisfaction. The SWLS has strong internal consistency (α = .87;
Diener et al., 1985), 2-month test-retest reliability (r = .82; Diener et al.,
1985), and convergent validity with other well-being measures (r = .50-.75;
Pavot & Diener, 1993).

Cronbach's α in the current sample = .88, consistent with normative α = .87
(Diener et al., 1985). Sample item: "In most ways my life is close to my
ideal." Full scale available in Diener et al. (1985) and at https://osf.io/abc456/.
```

**Improvement**: Full citation (Diener et al., 1985), 5 items, 7-point Likert, scoring (sum, 5-35 range), sample item, current reliability (α=.88), normative reliability (α=.87), convergent validity (r=.50-.75), test-retest reliability (r=.82), availability (original publication + OSF)

---

### Fix 7: Training Protocol Details

**❌ BEFORE** (Example 4.1):
```
The training group received cognitive reappraisal training, while the control
group received no intervention. Sessions lasted approximately 60 minutes.
```

**✅ AFTER** (Example 4.2):
```
Cognitive Reappraisal Training Protocol. We developed a 60-minute single-session
intervention based on established cognitive reappraisal protocols (Jamieson et al.,
2012; McRae et al., 2012). The intervention was delivered via pre-recorded video
presentation (standardized across participants to control for instructor effects)
accessible through Qualtrics (Version December 2024). Training materials and video
available at https://osf.io/def789/.

Training structure: (1) Psychoeducation on emotion regulation and reappraisal
(10 minutes; PowerPoint slides with narration); (2) Cognitive reappraisal techniques
demonstration (20 minutes; video examples of reframing negative events, perspective-
taking, distancing; based on Gross, 2002); (3) Guided practice with 5 standardized
scenarios (20 minutes; participants typed reappraisal responses, received automated
feedback based on keyword matching; adapted from Denny & Ochsner, 2014);
(4) Homework planning (10 minutes; participants created daily reappraisal diary
template). Training manual with all scenarios and instructions available at
https://osf.io/ghi012/.
```

**Improvement**: From vague "cognitive reappraisal training" to complete operationalization: Protocol basis (established studies), exact duration (60min = 10+20+20+10), delivery method (pre-recorded video via Qualtrics Dec 2024, controls instructor effects), content breakdown (4 components with timing), specific techniques (reframing, perspective-taking, distancing from Gross 2002), practice details (5 scenarios, typed responses, automated keyword feedback from Denny & Ochsner 2014), homework structure (daily diary), materials availability (OSF links for video + manual)

---

### Fix 8: Active Control Group

**❌ BEFORE** (Example 4.1):
```
control group received no intervention
```

**✅ AFTER** (Example 4.2):
```
Active Control Condition. To control for time, attention, and expectancy effects,
the control group viewed a 60-minute health documentary ("The Human Body: Systems,"
BBC, 2018) delivered via the same Qualtrics platform. Documentary content was
matched for engagement and audiovisual complexity but contained no emotion regulation
content (verified by two independent raters; κ = .89). Control participants also
completed daily logs (parallel structure to training group homework) but received
no review or feedback. Control materials available at https://osf.io/jkl345/.
```

**Improvement**: From "no intervention" (confounds: attention, expectancy, time) to **active control**: 60-min health documentary (BBC 2018, specific title), same platform (Qualtrics), content verification (2 raters confirmed no ER content, κ=.89 high agreement), engagement/complexity matched, parallel homework structure (daily logs, no feedback), materials available (OSF)

**Rationale**: Active controls eliminate alternative explanations that training effects are due to attention, time, or expectancy rather than specific content

---

### Fix 9: Post-Assessment Timing

**❌ BEFORE** (Example 4.1):
```
All participants completed post-intervention assessments two weeks later.
```

**✅ AFTER** (Example 4.2):
```
Session 3 (Post-Assessment, Day 14 ± 1). Exactly 14 days after the intervention
session (±1 day to accommodate participant availability), participants received
an email link to the post-assessment. Automated email reminders were sent at
Days 12 and 13 for non-responders. Post-assessment completion: 78/80 participants
(97.5%) completed on Day 14, 2 participants completed on Day 15. Session duration:
M = 10 minutes (SD = 2).
```

**Improvement**: From vague "two weeks later" to exact specification: Day 14 ± 1 (exact interval with tolerance for availability), email delivery, automated reminders (Days 12/13 for non-responders), compliance data (78/80 = 97.5% on Day 14, 2 on Day 15), duration (M=10min, SD=2)

---

### Fix 10: Data Analysis Software & Assumptions

**❌ BEFORE** (Example 4.1):
```
Data were analyzed using ANOVA to compare groups on emotion regulation and
well-being scores. Statistical significance was set at p < .05.
```

**✅ AFTER** (Example 4.2):
```
All analyses were conducted using R version 4.2.1 (R Core Team, 2022) with
RStudio 2022.07.1. Analysis code, raw data (with participant identifiers removed),
and materials are available at https://osf.io/mno678/.

[Preprocessing described with readr 2.1.2, missing data handling, outlier
treatment via winsorization >3 SD per Leys et al., 2013, sensitivity analyses]

We tested the hypothesis using 2 (Group: Training vs Control) × 2 (Time:
Baseline vs Post-Assessment) mixed-design ANOVAs for each outcome (ERQ-Reappraisal,
SWLS). We conducted ANOVAs using the aov() function from the stats package in R.

Assumption Testing Results. Shapiro-Wilk tests indicated no violations of normality
for any variable in either group at either timepoint (all ps > .05). Levene's tests
indicated no violations of homogeneity of variance for any variable (ERQ-Reappraisal:
F(1,78) = 1.45, p = .23; SWLS: F(1,78) = 0.89, p = .35). Mauchly's test indicated
sphericity was not violated for the Time factor (all ps > .05), so no corrections
were necessary.

Effect sizes were calculated as partial eta-squared (η²_p) for omnibus tests and
Cohen's d for pairwise comparisons using the effectsize package (version 0.8.0;
Ben-Shachar et al., 2020). We used conventional thresholds: η²_p = .01 (small),
.06 (medium), .14 (large); Cohen's d = 0.2 (small), 0.5 (medium), 0.8 (large)
(Cohen, 1988).

Statistical significance was set at α = .05 (two-tailed) for all tests. We did
not adjust for multiple comparisons across the two primary outcomes (ERQ-Reappraisal,
SWLS) because they tested distinct a priori hypotheses (Rothman, 1990), but we
reported exact p-values to allow readers to evaluate evidence strength.
```

**Improvement**: From generic "ANOVA" to complete specification:
- **Software**: R 4.2.1 + RStudio 2022.07.1 (versions specified)
- **Packages**: readr 2.1.2 (import), stats aov() (ANOVA), effectsize 0.8.0 (effect sizes)
- **Preprocessing**: Missing data (0.3%, listwise deletion), outliers (>3 SD winsorized per Leys et al., 2013, sensitivity analysis)
- **Analysis**: 2×2 mixed ANOVA (Group between × Time within) for ERQ-R and SWLS separately
- **Assumptions**: Normality (Shapiro-Wilk, all ps>.05), homogeneity (Levene, ERQ F=1.45 p=.23, SWLS F=0.89 p=.35), sphericity (Mauchly, all ps>.05, no corrections needed)
- **Effect sizes**: η²_p (omnibus) + d (pairwise) with thresholds (.01/.06/.14, 0.2/0.5/0.8 per Cohen 1988)
- **Inference**: α=.05 two-tailed, no MC adjustment (distinct a priori hypotheses per Rothman 1990), exact ps reported
- **Transparency**: Code + data + materials on OSF

---

## Key Lessons for Nature/Science-Level Methods

### 1. Reproducibility is NOT Optional

**Top-tier journals reject papers with reproducibility gaps**, even if results are interesting.

**Gold Standard**: A researcher who has never met you should be able to:
- ✅ Recruit a comparable sample (recruitment method + criteria)
- ✅ Obtain the exact same measures (scale names + versions + OSF links)
- ✅ Administer the identical protocol (step-by-step with timing)
- ✅ Run the same analysis (software + versions + code)
- ✅ Verify your results (raw data available, assumptions reported)

**Self-Check**: If you can't answer "How would someone replicate this?" for EVERY sentence in your Methods → revise

---

### 2. Active Controls > No-Intervention Controls

**Problem with "no intervention"**: Confounds time, attention, expectancy effects

**Solution**: Design active controls that match:
- ⏱️ Time commitment (60 min documentary = 60 min training)
- 👀 Attention demands (video viewing = video watching)
- 📝 Homework structure (daily logs = daily logs)
- BUT: No active ingredient (health content ≠ emotion regulation content)

**Verification**: Have independent raters confirm control contains no active ingredient (Example 4.2: κ=.89 for "no ER content")

---

### 3. Transparent Assumption Testing

**Don't just report "we used ANOVA"** → Show you checked if ANOVA was appropriate

**Example 4.2 demonstrates**:
- ✅ Normality tested (Shapiro-Wilk per group per timepoint)
- ✅ Homogeneity tested (Levene's test with F-statistic + p-value)
- ✅ Sphericity tested (Mauchly's test)
- ✅ Corrections planned (Greenhouse-Geisser if needed)
- ✅ Results reported (all assumptions met with exact statistics)

**Why this matters**: Reviewers want to see you're not blindly applying tests

---

### 4. Pre-Registration and Transparency

**Example 4.2 shows**:
- A priori power analysis (not post-hoc)
- Planned oversampling (10% buffer)
- Exclusion criteria stated upfront (not data-dependent)
- No multiple comparison adjustment justified (distinct a priori hypotheses, Rothman 1990)
- Sensitivity analyses for outlier treatment

**Transparency markers**:
- "We did not adjust for multiple comparisons... **because they tested distinct a priori hypotheses**"
- "Outliers were winsorized... **Sensitivity analyses showed results were unchanged**"
- "We oversampled... **to account for anticipated 10% attrition**"

These statements show you **made deliberate, defensible choices** (not p-hacking)

---

### 5. Complete Material Specification

**For EVERY measure, include**:
1. ✅ Full citation (Author, Year)
2. ✅ Number of items + subscales
3. ✅ Response format (e.g., 7-point Likert, 1-7)
4. ✅ Scoring procedure (average vs sum, range)
5. ✅ Sample item (at least 1 per subscale)
6. ✅ Reliability: Current sample + normative (α with citations)
7. ✅ Validity evidence (convergent, test-retest with r-values)
8. ✅ Availability (citation + OSF/repository link)

**Example 4.2 does this for ERQ and SWLS** - compare to Example 4.1's vague "questionnaires"

---

### 6. Step-by-Step Procedure with Timing

**Don't write**: "Participants completed measures at baseline and 2 weeks later"

**Do write** (Example 4.2 model):
- **Session 1 (Day 0)**: E-consent → Demographics → ERQ/SWLS (randomized order) → 3 IMCs → M=12min
- **Session 2 (Days 1-3, M=1.8, SD=0.9)**: Email link → One sitting → Quiet + headphones → Training (M=64min) vs Control (M=62min, ns) → Homework instructions (14 days, reminders Days 4/7/10/13)
- **Session 3 (Day 14±1)**: Email → Same measures (randomized) → Reminders Days 12/13 → 97.5% on Day 14 → M=10min

**Key elements**: Timing (exact days), delivery method (email, Qualtrics), environment (one sitting, quiet, headphones), duration (M + SD), compliance (97.5% on Day 14)

---

### 7. Software Versions and Code Availability

**Minimum for top-tier journals**:
- Software name + version (R 4.2.1, not just "R")
- Packages + versions (effectsize 0.8.0, not just "effectsize")
- Specific functions (aov() from stats, not just "ANOVA")
- Repository links (OSF/GitHub for code + data)

**Example 4.2**: R 4.2.1, RStudio 2022.07.1, readr 2.1.2, effectsize 0.8.0, mediation 4.5.0, G*Power 3.1.9.7, code at https://osf.io/mno678/

**Why**: Computational reproducibility requires exact versions (functions change across versions)

---

### 8. Effect Sizes are Mandatory

**Never report** p-values alone

**Always report** effect sizes with conventional thresholds:
- Omnibus tests: η²_p with .01/.06/.14 (small/medium/large)
- Pairwise tests: Cohen's d with 0.2/0.5/0.8 (small/medium/large)
- Include 95% CIs when possible

**Example 4.2**: "Effect sizes were calculated as partial eta-squared (η²_p) for omnibus tests and Cohen's d for pairwise comparisons using the effectsize package (version 0.8.0; Ben-Shachar et al., 2020)."

---

### 9. Handling Outliers and Missing Data

**Don't ignore** - explicitly state your approach

**Example 4.2 demonstrates**:
- **Detection rule**: >3 SD from group mean per measure per timepoint
- **Prevalence**: 3 outliers (3.8% of data points)
- **Treatment**: Winsorized to 3 SD (Leys et al., 2013 recommendation for small samples)
- **Sensitivity**: Results unchanged when outliers excluded entirely (reported in Supplement)
- **Missing data**: 0.3% item-level, listwise deletion for 2 incomplete post-assessments

**Transparency**: State rule → report prevalence → justify treatment → verify robustness

---

### 10. Justifying Statistical Decisions

**Example 4.2 shows how to justify**:

**No multiple comparison correction**:
> "We did not adjust for multiple comparisons across the two primary outcomes
> (ERQ-Reappraisal, SWLS) because they tested distinct a priori hypotheses
> (Rothman, 1990), but we reported exact p-values to allow readers to evaluate
> evidence strength."

**Winsorization over exclusion**:
> "Following recommendations for small samples (Leys et al., 2013), outliers were
> winsorized to 3 SD from the mean rather than excluded. Sensitivity analyses
> (reported in Supplementary Materials) showed results were unchanged when
> outliers were excluded entirely."

**Sample size**:
> "Sample size was determined a priori using G*Power 3.1.9.7... based on previous
> reappraisal training studies (Jamieson et al., 2012; McRae et al., 2012)"

**Pattern**: State decision → Provide evidence/citation → Show robustness/alternatives

---

## Practice Exercise

### Your Turn: Transform YOUR Methods from 6/30 to 30/30

**Step 1**: Read your current Methods section

**Step 2**: Use Template 4.3 (Reproducibility Checklist) to score each element (0-5)

**Step 3**: Identify elements scoring <5

**Step 4**: For each gap, write "Before → After" using Example 4.2 as a model

**Example Process**:

```
My Element 2 (Materials) Score: 2/5

What's MISSING:
- No scale version specified
- No reliability data (current or normative)
- No sample items
- No availability links

BEFORE: "We used the Beck Depression Inventory to measure depression."

AFTER: "Beck Depression Inventory-II (BDI-II; Beck et al., 1996). The BDI-II
is a 21-item self-report measure of depression severity (e.g., 'I feel sad
much of the time'). Participants responded on a 4-point scale (0-3) for each
item. Total scores were calculated by summing all items (range: 0-63), with
higher scores indicating greater depression severity (0-13 minimal, 14-19 mild,
20-28 moderate, 29-63 severe; Beck et al., 1996). Cronbach's α in the current
sample = .91, consistent with normative α = .92 (Beck et al., 1996). The
BDI-II demonstrates high convergent validity with clinician ratings (r = .83;
Beck et al., 1996). Full scale available in Beck et al. (1996) and at
https://osf.io/xyz123/."

New Score: 5/5 ✅
```

**Goal**: Achieve ≥27/30 (90%) for Nature/Science submission readiness

---

## Self-Check Questions

Before claiming your Methods is "reproducible," ask:

1. ❓ **Participant Replication**: Could a researcher recruit a comparable sample using only my description?
   - If NO → Add recruitment method, source, timing, inclusion/exclusion criteria, demographics

2. ❓ **Material Acquisition**: Could someone obtain the exact same measures/materials?
   - If NO → Add full citations, versions, psychometrics, sample items, repository links

3. ❓ **Procedure Replication**: Could someone run the identical protocol step-by-step?
   - If NO → Add chronological steps with exact timing, delivery methods, environmental controls

4. ❓ **Analysis Verification**: Could someone reproduce your exact analysis and verify results?
   - If NO → Add software versions, preprocessing rules, assumption tests, outlier handling, code/data links

5. ❓ **Statistical Justification**: Can you defend EVERY statistical choice with evidence?
   - If NO → Add power analysis, effect sizes, assumption tests, sensitivity analyses, citations

6. ❓ **Control Strategy**: Are alternative explanations adequately ruled out?
   - If NO → Use active controls (not "no intervention"), match time/attention/expectancy

**If you answer NO to any question → Your Methods needs revision using Example 4.2 as a template**

---

## Comparison Summary: What Makes Methods "Good"?

| Aspect | Bad Methods (4.1) | Good Methods (4.2) |
|--------|-------------------|-------------------|
| **Participants** | Vague, incomplete | Recruitment + criteria + demographics + power + IRB |
| **Materials** | Generic "questionnaires" | Full citations + psychometrics + sample items + links |
| **Procedure** | General flow only | Step-by-step chronology with timing + environment |
| **Parameters** | Vague "approximately" | Exact values + units + ranges for all parameters |
| **Software** | Not mentioned | Names + versions + specific functions + code links |
| **Analysis** | Generic "ANOVA" | Complete: software + preprocessing + assumptions + effect sizes + justifications + transparency |
| **Controls** | "No intervention" | Active control (time/attention/expectancy matched, content verified) |
| **Transparency** | Minimal | Sensitivity analyses + exact statistics + data/code sharing |
| **Reproducibility Score** | 6/30 (20%) ❌ | 30/30 (100%) ✅ |
| **Top-tier Publishability** | REJECT | ACCEPT |

---

## Related Materials

**Templates**:
- `template_4.1_bulletproofing_audit_canvas.md`: AI-assisted audit for Top 10 rejection reasons
- `template_4.3_reproducibility_checklist.md`: Detailed 6-element scoring (0-5 each, 30 total)
- `template_4.4_peer_review_rubric.md`: Comprehensive peer review integrating all dimensions

**Examples**:
- `example_4.1_bad_methods.md`: Demonstrates critical reproducibility gaps (6/30 score)
- `example_4.3_bad_results.md`: Overclaiming and weak statistics (coming next)
- `example_4.4_good_results.md`: Appropriate claims and rigorous statistics (coming next)

**Lecture Notes**:
- `week4/lecture_notes.md`: Complete bulletproofing strategies (Top 10 reasons, reproducibility, controls, statistics)

**Recipes**:
- Recipe #35: Reproducibility Vulnerability Scanner
- Recipe #36: Alternative Explanation Generator
- Recipe #37: Power Analysis Reviewer

---

## Usage in Workshop

**Template 4.3 (Reproducibility Checklist) Application**:
1. Students bring their Methods sections
2. Use checklist to score each of 6 elements (0-5)
3. Compare their gaps to Example 4.1's gaps (identify common patterns)
4. Use Example 4.2 as a model to fix their top 3 gaps
5. Peer review revised Methods using Template 4.4

**Learning Sequence**:
- **Before Workshop**: Read Example 4.1 (bad) + Example 4.2 (good), identify 10 differences
- **During Workshop**: Score your Methods (Template 4.3), write 3 Before→After fixes using 4.2 as model
- **After Workshop**: Revise entire Methods to ≥27/30, submit for peer review (Template 4.4)

**Expected Outcome**: Students transform their Methods from 15-20/30 (weak) to 27-30/30 (excellent) by systematically addressing all reproducibility elements

---

**Example Version**: 1.0
**Last Updated**: 2025-01-09
**Use**: Week 4 workshop - Model for complete reproducibility and Nature/Science-level Methods writing
