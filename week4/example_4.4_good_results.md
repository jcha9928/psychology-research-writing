# Example 4.4: Good Results Section (Complete Statistical Rigor)

**Purpose**: Demonstrate Nature/Science-level Results with complete effect sizes, CIs, and appropriate claims

**Use in Workshop**: Model for Template 4.1 (Bulletproofing Audit) and comparison with Example 4.3

---

## Research Topic

**Domain**: Social Psychology - Emotion Regulation and Well-Being
**Study Type**: Experimental intervention (cognitive reappraisal training)
**Same study as Examples 4.1-4.3, showing all Results best practices applied**

---

## Good Results Section (Bulletproofed)

```
Results

Preliminary Analyses

Baseline Equivalence. Independent-samples t-tests confirmed that the training
and control groups did not differ significantly at baseline on reappraisal use
(training: M = 4.18, SD = 0.82; control: M = 4.12, SD = 0.74; t(78) = 0.35,
p = .73, d = 0.08, 95% CI [-0.36, 0.52]) or life satisfaction (training:
M = 22.05, SD = 5.18; control: M = 21.00, SD = 4.87; t(78) = 0.94, p = .35,
d = 0.21, 95% CI [-0.23, 0.65]). This supports the effectiveness of random
assignment in creating equivalent groups prior to the intervention.

Descriptive Statistics. Table 1 presents means, standard deviations, standard
errors, and 95% confidence intervals for all measures by group and time. Figure 1
illustrates the Group × Time interaction patterns.

---

Table 1
Descriptive Statistics for Reappraisal and Life Satisfaction by Group and Time

Measure                Group      Time        M      SD    SE    95% CI           n
─────────────────────────────────────────────────────────────────────────────────
Reappraisal (ERQ)      Training   Baseline   4.18   0.82  0.13  [3.92, 4.44]   40
                                  Post       4.76   0.88  0.14  [4.48, 5.04]   40
                                  Change     0.58   0.72  0.11  [0.35, 0.81]   40

                       Control    Baseline   4.12   0.74  0.12  [3.88, 4.36]   40
                                  Post       4.23   0.79  0.13  [3.98, 4.48]   40
                                  Change     0.11   0.68  0.11  [-0.11, 0.33]  40

Life Satisfaction      Training   Baseline  22.05   5.18  0.82  [20.40, 23.70]  40
(SWLS)                            Post      24.80   5.76  0.91  [22.96, 26.64]  40
                                  Change     2.75   4.12  0.65  [1.43, 4.07]   40

                       Control    Baseline  21.00   4.87  0.77  [19.44, 22.56]  40
                                  Post      21.25   5.02  0.79  [19.65, 22.85]  40
                                  Change     0.25   3.89  0.62  [-0.99, 1.49]  40

Note. ERQ = Emotion Regulation Questionnaire (range: 1-7, higher scores indicate
greater reappraisal use). SWLS = Satisfaction With Life Scale (range: 5-35, higher
scores indicate greater life satisfaction). Change = Post - Baseline. 95% CI =
95% confidence interval for the mean. SE = standard error of the mean.
─────────────────────────────────────────────────────────────────────────────────

---

Figure 1. Mean reappraisal use (ERQ scores) and life satisfaction (SWLS scores)
by Group (Training vs Control) and Time (Baseline vs Post-Assessment). Error bars
represent 95% confidence intervals. Panel A shows that the training group exhibited
significantly greater increase in reappraisal use compared to the control group
(Group × Time interaction: F(1,78) = 4.35, p = .04, η²_p = .05). Panel B shows
that the Group × Time interaction for life satisfaction did not reach statistical
significance (F(1,78) = 3.89, p = .052, η²_p = .05).

---

Primary Analyses

Assumption Testing. Shapiro-Wilk tests confirmed normality for all variables in
both groups at both timepoints (all ps > .05). Levene's tests confirmed homogeneity
of variance for reappraisal (F(1,78) = 1.12, p = .29) and life satisfaction
(F(1,78) = 0.76, p = .39). Mauchly's tests confirmed sphericity for the repeated
measures factor (all ps > .05), so no Greenhouse-Geisser corrections were needed.

Three outliers (>3 SD from group mean) were identified (3.8% of data points:
2 on baseline reappraisal in the training group, 1 on post-assessment life
satisfaction in the control group). Following recommendations for small samples
(Leys et al., 2013), outliers were winsorized to 3 SD from the mean. Sensitivity
analyses (Supplementary Table S1) showed that results were unchanged when outliers
were excluded entirely, supporting the robustness of findings.

Reappraisal Use. We conducted a 2 (Group: Training vs Control) × 2 (Time: Baseline
vs Post-Assessment) mixed-design ANOVA on reappraisal use (ERQ scores). The main
effect of Group was not significant, F(1,78) = 1.89, p = .17, η²_p = .02, indicating
no overall difference in reappraisal use between groups when collapsing across time.
The main effect of Time was significant, F(1,78) = 12.45, p < .001, η²_p = .14
(large effect), indicating an overall increase in reappraisal from baseline to
post-assessment when collapsing across groups.

Critically, the Group × Time interaction was statistically significant, F(1,78) = 4.35,
p = .04, η²_p = .05 (small-to-medium effect), indicating that the change in
reappraisal use from baseline to post-assessment differed between groups. To
decompose this interaction, we conducted post-hoc paired-samples t-tests within
each group (Bonferroni-corrected α = .05/2 = .025 for two comparisons).

In the training group, reappraisal use increased significantly from baseline
(M = 4.18, SD = 0.82) to post-assessment (M = 4.76, SD = 0.88), t(39) = 5.09,
p < .001, d = 0.81, 95% CI [0.48, 1.14] (large effect). This represents a
mean increase of 0.58 points on the 7-point ERQ scale (8.3% of the total scale
range), with a 95% CI for the change of [0.35, 0.81].

In the control group, reappraisal use showed a small, non-significant increase
from baseline (M = 4.12, SD = 0.74) to post-assessment (M = 4.23, SD = 0.79),
t(39) = 1.02, p = .31, d = 0.16, 95% CI [-0.15, 0.47] (trivial effect). The
mean increase was 0.11 points (1.6% of scale range), with a 95% CI for the
change of [-0.11, 0.33].

The difference in change scores between groups was significant: training group
change (M = 0.58, SD = 0.72) exceeded control group change (M = 0.11, SD = 0.68)
by 0.47 points, t(78) = 2.97, p = .004, d = 0.67, 95% CI [0.22, 1.11] (medium-to-large
effect). The 95% CI for the group difference in change was [0.15, 0.79], indicating
that the true training advantage likely ranges from 0.15 to 0.79 points on the
7-point scale.

Life Satisfaction. We conducted a 2 (Group: Training vs Control) × 2 (Time:
Baseline vs Post-Assessment) mixed-design ANOVA on life satisfaction (SWLS scores).
The main effect of Group was not significant, F(1,78) = 1.67, p = .20, η²_p = .02.
The main effect of Time was significant, F(1,78) = 8.93, p = .004, η²_p = .10
(medium effect), indicating an overall increase in life satisfaction from baseline
to post-assessment.

The Group × Time interaction did not reach statistical significance, F(1,78) = 3.89,
p = .052, η²_p = .05 (small-to-medium effect). While the direction of effects was
consistent with predictions (training group showed larger increase than control
group), this finding should be interpreted with caution given the p-value exceeded
our pre-specified α = .05 threshold.

For completeness, we report exploratory post-hoc comparisons (not corrected for
multiple comparisons given the non-significant omnibus test). In the training group,
life satisfaction increased from baseline (M = 22.05, SD = 5.18) to post-assessment
(M = 24.80, SD = 5.76), t(39) = 4.22, p < .001, d = 0.67, 95% CI [0.33, 1.01]
(medium-to-large effect), representing a mean increase of 2.75 points (9.2% of
the 5-35 scale range). In the control group, life satisfaction showed a small,
non-significant increase from baseline (M = 21.00, SD = 4.87) to post-assessment
(M = 21.25, SD = 5.02), t(39) = 0.41, p = .69, d = 0.06, 95% CI [-0.25, 0.37]
(trivial effect), with a mean increase of 0.25 points (0.8% of scale range).

The difference in change scores between groups approached but did not reach
statistical significance: training group change (M = 2.75, SD = 4.12) exceeded
control group change (M = 0.25, SD = 3.89) by 2.50 points, t(78) = 2.78, p = .007,
d = 0.62, 95% CI [0.18, 1.07] (medium effect). However, given the non-significant
omnibus interaction (p = .052), this pairwise comparison should be considered
exploratory and hypothesis-generating rather than confirmatory.

Post-Hoc Power Analysis. Given the non-significant result for life satisfaction,
we conducted post-hoc power analyses using G*Power 3.1.9.7 (Faul et al., 2007).
For the observed effect size (η²_p = .05, equivalent to f = 0.23), our sample
(N = 80, n = 40 per group) provided 51% power at α = .05 (two-tailed). To achieve
80% power for detecting an effect of this magnitude, we would require N = 146
(n = 73 per group). This suggests the null result may reflect insufficient power
rather than true absence of an effect, warranting replication with larger samples.

Secondary Analyses

Mediation Analysis. To test whether increased reappraisal use mediated the
relationship between training condition and life satisfaction change, we conducted
a mediation analysis using the mediation package (version 4.5.0; Tingley et al.,
2014) in R. We regressed life satisfaction change scores on reappraisal change
scores and group (0 = control, 1 = training), testing the indirect effect of
group on life satisfaction through reappraisal change. We estimated 95% confidence
intervals using bias-corrected bootstrap with 5,000 iterations.

The total effect of group on life satisfaction change was significant (b = 2.50,
SE = 0.90, p = .007, 95% CI [0.71, 4.29]), indicating that the training group
showed greater life satisfaction increase than the control group. The direct effect
of group on life satisfaction change (after controlling for reappraisal change)
was reduced and non-significant (b = 1.85, SE = 0.92, p = .048, 95% CI [0.03, 3.67]),
suggesting partial mediation. The indirect effect through reappraisal change was
small but significant (b = 0.65, SE = 0.31, 95% CI [0.12, 1.35], excluding zero),
accounting for 26% of the total effect.

Reappraisal change positively predicted life satisfaction change (b = 1.38,
SE = 0.52, p = .01, 95% CI [0.35, 2.41]), indicating that each 1-point increase
in reappraisal use was associated with a 1.38-point increase in life satisfaction,
controlling for group. This provides preliminary evidence that increased reappraisal
use may be one mechanism through which training affects well-being, though the
small effect size and wide confidence interval suggest this relationship is modest
and requires replication.

Exploratory Analyses. We conducted exploratory correlational analyses to examine
potential moderators of training effects. Age showed a weak positive correlation
with baseline reappraisal use (r = .18, p = .11, 95% CI [-0.04, 0.39], r² = .03),
accounting for only 3% of variance. Given the narrow age range in our sample
(18-24 years, M = 19.2, SD = 1.4), this correlation likely has limited
generalizability and may reflect Type I error. Gender was not significantly
associated with baseline reappraisal (point-biserial r = .12, p = .28,
95% CI [-0.10, 0.34]) or life satisfaction (r = .08, p = .48, 95% CI [-0.14, 0.30]).

We did not correct for multiple comparisons in these exploratory analyses because
they were not pre-specified hypotheses (Rothman, 1990). However, readers should
consider the inflated family-wise error rate when evaluating these findings. With
4 exploratory tests (age-reappraisal, age-life satisfaction, gender-reappraisal,
gender-life satisfaction), the probability of at least one Type I error is
approximately 18.5% (1 - (1-.05)^4 = .185). We report all tests conducted to
avoid selective reporting; non-significant results are included in Supplementary
Table S2.

Practical Significance

Minimal Important Difference Benchmarks. To evaluate the clinical or practical
significance of our findings, we compared observed effect sizes to established
benchmarks. For the ERQ, no consensus minimal important difference (MID) exists
in the literature. However, our training effect (d = 0.67 for group difference
in change) exceeds the conventional medium effect threshold (d = 0.50; Cohen, 1988)
and is comparable to meta-analytic estimates for brief emotion regulation
interventions (d = 0.65; Webb et al., 2012), suggesting the magnitude is consistent
with prior research.

For the SWLS, Diener et al. (1985) suggest that a 5-point change represents
a "noticeable" shift in life satisfaction based on normative data. Our training
group showed a mean increase of 2.75 points (95% CI [1.43, 4.07]), which is
below this MID threshold. This suggests that while the effect was statistically
significant in exploratory analyses (p = .007 for group difference in change),
it may not represent a clinically meaningful improvement in subjective well-being.
The upper bound of the 95% CI (4.07) approaches but does not consistently exceed
the MID, indicating that some participants may have experienced meaningful change
while others did not.

Comparison to Meta-Analytic Effects. A recent meta-analysis of cognitive reappraisal
training studies (Malooly et al., 2013) reported a mean effect size of d = 0.71
for improvements in emotion regulation ability. Our observed effect (d = 0.67)
falls within the 95% CI of this meta-analytic estimate ([0.52, 0.90]), suggesting
our findings are consistent with the broader literature. For well-being outcomes,
the meta-analysis reported smaller effects (d = 0.42 on average), with substantial
heterogeneity across studies (I² = 68%). Our exploratory effect for life
satisfaction (d = 0.62) is numerically larger but overlaps with the meta-analytic
distribution, and the non-significant omnibus test (p = .052) suggests this effect
should be interpreted cautiously pending replication.

Limitations of Current Findings. Several limitations constrain the interpretation
of practical significance. First, our measures are self-report questionnaires
assessing beliefs about emotion regulation (ERQ) and global life satisfaction
(SWLS), not behavioral or physiological indicators of regulatory success or
momentary well-being. Second, the brief single-session training format may produce
smaller effects than multi-session interventions, limiting the magnitude of change
achievable. Third, the 2-week follow-up is relatively short; long-term maintenance
of effects is unknown. Fourth, our sample consisted of first- and second-year
undergraduate psychology students at a single university, limiting generalizability
to other populations (e.g., clinical samples, older adults, non-Western cultures).
Finally, we cannot rule out alternative explanations for training effects
(e.g., expectancy, attention, demand characteristics) without active control
manipulation checks assessing awareness of study hypotheses.

Summary. Participants who received brief cognitive reappraisal training showed
significantly greater increases in self-reported reappraisal use compared to
controls, with a medium-to-large effect size (d = 0.67) consistent with prior
meta-analytic estimates. The effect on life satisfaction was non-significant
at the omnibus level (p = .052), though exploratory pairwise comparisons suggested
a potential medium effect (d = 0.62) that fell below established minimal important
difference thresholds. Mediation analyses provided preliminary evidence that
increased reappraisal use may partially account for training effects on well-being,
though the small indirect effect (26% of total) and wide confidence intervals
indicate this mechanism is modest and uncertain. Replication with adequately
powered samples (N ≥ 146 per group), behavioral outcomes, active control
manipulation checks, and diverse populations is needed to confirm these findings
and evaluate their practical significance and generalizability.
```

---

## Bulletproofing Scorecard: All Top 10 Rejection Reasons Addressed

### ✅ Rejection Reason #6: Overclaiming - ADDRESSED

**Example 4.3 Problem**: "prove", "demonstrates", causal language

**Example 4.4 Solution**:
- Uses qualified language: "provide preliminary evidence", "suggest", "consistent with"
- Acknowledges limitations proactively: "alternative explanations cannot be ruled out"
- Distinguishes confirmatory (reappraisal) from exploratory (life satisfaction) findings
- Compares to meta-analytic benchmarks rather than overstating novelty

**Key Phrases**:
- "This provides preliminary evidence that increased reappraisal use **may be one mechanism**..."
- "Replication with adequately powered samples... **is needed to confirm** these findings"
- "Several limitations **constrain the interpretation** of practical significance"

---

### ✅ Rejection Reason #7: Cherry-Picking - ADDRESSED

**Example 4.3 Problem**: p = .052 treated as "marginally significant", weak correlations hyped

**Example 4.4 Solution**:
- Treats p = .052 honestly as non-significant: "did not reach statistical significance"
- Reports all exploratory tests (age, gender correlations) in Supplementary Table S2
- Acknowledges family-wise error rate (18.5% with 4 tests)
- States "we report all tests conducted to avoid selective reporting"
- Provides post-hoc power analysis explaining null result (51% power)

**Key Phrases**:
- "The Group × Time interaction **did not reach statistical significance** (p = .052)"
- "This finding should be interpreted with caution given the p-value **exceeded our pre-specified α = .05 threshold**"
- "We report **all tests conducted** to avoid selective reporting; non-significant results are included in Supplementary Table S2"

---

### ✅ Rejection Reason #8: Statistical Issues - ADDRESSED

**Example 4.3 Problem**: No effect sizes, no CIs, incomplete descriptives, no assumption tests

**Example 4.4 Solution**:
- **Effect sizes for EVERY test**: η²_p for ANOVAs, Cohen's d for t-tests, r for correlations
- **95% CIs for all estimates**: means (Table 1), effect sizes, group differences
- **Complete descriptive statistics**: M, SD, SE, 95% CI, n (Table 1)
- **Assumption testing reported**: Shapiro-Wilk (normality), Levene (homogeneity), Mauchly (sphericity) with exact statistics
- **Outlier handling transparent**: 3 outliers winsorized, sensitivity analyses in Supplement
- **Multiple comparison adjustment**: Bonferroni for post-hoc pairwise (α = .025), no correction for primary tests with justification

**Example Statistics Reported**:
```
Training group change: d = 0.81, 95% CI [0.48, 1.14] (large effect)
Group difference: d = 0.67, 95% CI [0.22, 1.11] (medium-to-large effect)
Group × Time interaction: F(1,78) = 4.35, p = .04, η²_p = .05 (small-to-medium)
Levene's test: F(1,78) = 1.12, p = .29 (homogeneity confirmed)
Shapiro-Wilk: all ps > .05 (normality confirmed)
```

---

### ✅ Rejection Reason #9: Unclear Presentation - ADDRESSED

**Example 4.3 Problem**: No table, no figure, all results in-text

**Example 4.4 Solution**:
- **Table 1**: Complete descriptive statistics (M, SD, SE, 95% CI, n) for all measures by Group × Time
- **Figure 1 description**: Interaction plots with 95% CI error bars for visual clarity
- **Supplementary Table S1**: Sensitivity analyses for outlier treatment
- **Supplementary Table S2**: All exploratory tests (including non-significant)
- **Structured sections**: Preliminary Analyses → Primary Analyses → Secondary Analyses → Exploratory Analyses → Practical Significance → Summary

**Table 1 Features**:
- Includes baseline, post, and CHANGE scores (critical for understanding intervention effects)
- 95% CIs for all means (precision)
- Sample sizes per cell (verification)
- Clear note explaining scale ranges and abbreviations

---

### ✅ Rejection Reason #10: Weak Effect Sizes Hyped - ADDRESSED

**Example 4.3 Problem**: Small effects presented as large, no MID benchmarks, no meta-analytic comparison

**Example 4.4 Solution**:
- **Standardized effect sizes with interpretation**: d = 0.67 labeled "medium-to-large", d = 0.16 labeled "trivial"
- **Minimal Important Difference (MID) comparison**: SWLS change (2.75) compared to MID (5.0) → below threshold
- **Meta-analytic benchmarks**: d = 0.67 compared to Webb et al. (2012) meta-analysis d = 0.65 → consistent
- **Practical significance section**: Dedicated analysis of clinical meaningfulness
- **Percentage of scale range**: 0.58 points on 7-point scale = 8.3% of range (provides context)
- **Limitations acknowledged**: Self-report measures, brief intervention, short follow-up

**Example Practical Significance Analysis**:
```
"For the SWLS, Diener et al. (1985) suggest that a 5-point change represents
a 'noticeable' shift in life satisfaction. Our training group showed a mean
increase of 2.75 points (95% CI [1.43, 4.07]), which is below this MID threshold.
This suggests that while the effect was statistically significant in exploratory
analyses (p = .007), it may not represent a clinically meaningful improvement
in subjective well-being."
```

---

## Comparison Table: Bad (Example 4.3) vs Good (Example 4.4)

| Element | Bad Results (4.3) | Good Results (4.4) | Improvement |
|---------|------------------|-------------------|-------------|
| **Effect Sizes** | ❌ None reported | ✅ η²_p + d + 95% CIs for ALL tests | Added: η²_p (.05), d (0.67-0.81), r (.18) with interpretations (small/medium/large) |
| **Confidence Intervals** | ❌ None | ✅ 95% CIs for all means, differences, effect sizes | Added: 95% CIs for M (Table 1), d ([0.48, 1.14]), group diff ([0.15, 0.79]) |
| **Descriptives** | ❌ M only, no SD/SE/n | ✅ M, SD, SE, 95% CI, n in Table 1 | Complete table with all Group × Time cells |
| **p = .052 Treatment** | ❌ "Marginally significant... clear trend" | ✅ "Did not reach significance (p = .052)... interpret with caution" | Honest null result + post-hoc power (51%) explaining why |
| **Overclaiming** | ❌ "Prove", "demonstrates", causal | ✅ "Preliminary evidence", "may be", "suggest" | Qualified claims matching data strength |
| **Table/Figure** | ❌ None | ✅ Table 1 (descriptives) + Figure 1 description | Visual summary for easy comparison |
| **Assumptions** | ❌ Not reported | ✅ Shapiro-Wilk, Levene, Mauchly with exact statistics | All assumptions tested and reported (normality, homogeneity, sphericity) |
| **Multiple Comparisons** | ❌ Ignored (4+ tests, no correction) | ✅ Bonferroni for post-hoc (α=.025), justified no correction for primary | Transparent about FWER (18.5% with 4 exploratory tests) |
| **Practical Significance** | ❌ Not addressed | ✅ MID benchmarks (SWLS 5.0), meta-analysis (d=0.65), % of scale range (8.3%) | Dedicated section comparing to clinical thresholds |
| **Cherry-Picking** | ❌ Only favorable results | ✅ All tests in Supp Table S2, null results acknowledged | Transparent reporting of all analyses |
| **Mediation** | ❌ Not tested | ✅ Indirect effect b=0.65, 95% CI [0.12, 1.35], 26% of total | Tests mechanism, quantifies mediation strength |
| **Limitations** | ❌ None mentioned in Results | ✅ Self-report, brief intervention, short follow-up, confounds acknowledged | Proactive limitation acknowledgment |
| **Power** | ❌ Not addressed | ✅ Post-hoc power 51%, need N=146 for 80% power | Explains null result, guides replication |

---

## Before → After Transformations (All Fixes Applied)

### Fix 1: From "Prove" to "Preliminary Evidence"

**❌ BEFORE** (Example 4.3):
```
These findings prove that cognitive reappraisal training is an effective
intervention for enhancing well-being in college students.
```

**✅ AFTER** (Example 4.4):
```
Participants who received brief cognitive reappraisal training showed
significantly greater increases in self-reported reappraisal use compared to
controls, with a medium-to-large effect size (d = 0.67) consistent with prior
meta-analytic estimates. The effect on life satisfaction was non-significant
at the omnibus level (p = .052), though exploratory pairwise comparisons
suggested a potential medium effect (d = 0.62) that fell below established
minimal important difference thresholds. Replication with adequately powered
samples (N ≥ 146 per group), behavioral outcomes, active control manipulation
checks, and diverse populations is needed to confirm these findings and
evaluate their practical significance and generalizability.
```

**Improvement**: Replaces "prove" with qualified claims, acknowledges null finding, compares to MID, states replication needs

---

### Fix 2: From "Marginally Significant" to Honest Null Result

**❌ BEFORE** (Example 4.3):
```
For life satisfaction, we also found a significant Group × Time interaction,
F(1,78) = 3.89, p = .052. Although this was marginally significant, it still
shows a clear trend toward improved well-being...
```

**✅ AFTER** (Example 4.4):
```
The Group × Time interaction did not reach statistical significance,
F(1,78) = 3.89, p = .052, η²_p = .05 (small-to-medium effect). While the
direction of effects was consistent with predictions (training group showed
larger increase than control group), this finding should be interpreted with
caution given the p-value exceeded our pre-specified α = .05 threshold.

[Later section:]
Post-Hoc Power Analysis. Given the non-significant result for life satisfaction,
we conducted post-hoc power analyses using G*Power 3.1.9.7 (Faul et al., 2007).
For the observed effect size (η²_p = .05, equivalent to f = 0.23), our sample
(N = 80, n = 40 per group) provided 51% power at α = .05 (two-tailed). To achieve
80% power for detecting an effect of this magnitude, we would require N = 146
(n = 73 per group). This suggests the null result may reflect insufficient power
rather than true absence of an effect, warranting replication with larger samples.
```

**Improvement**: Treats p = .052 as null result, provides post-hoc power analysis (51%), calculates required N for replication (146)

---

### Fix 3: From No Effect Sizes to Complete Effect Size Reporting

**❌ BEFORE** (Example 4.3):
```
The ANOVA revealed a significant Group × Time interaction for cognitive
reappraisal, F(1,78) = 4.35, p = .04. The training group showed increased
reappraisal use from baseline (M = 4.2) to post-assessment (M = 4.8), while
the control group remained unchanged (baseline M = 4.1, post-assessment M = 4.2).
```

**✅ AFTER** (Example 4.4):
```
Critically, the Group × Time interaction was statistically significant,
F(1,78) = 4.35, p = .04, η²_p = .05 (small-to-medium effect), indicating that
the change in reappraisal use from baseline to post-assessment differed between
groups.

In the training group, reappraisal use increased significantly from baseline
(M = 4.18, SD = 0.82) to post-assessment (M = 4.76, SD = 0.88), t(39) = 5.09,
p < .001, d = 0.81, 95% CI [0.48, 1.14] (large effect). This represents a
mean increase of 0.58 points on the 7-point ERQ scale (8.3% of the total scale
range), with a 95% CI for the change of [0.35, 0.81].

In the control group, reappraisal use showed a small, non-significant increase
from baseline (M = 4.12, SD = 0.74) to post-assessment (M = 4.23, SD = 0.79),
t(39) = 1.02, p = .31, d = 0.16, 95% CI [-0.15, 0.47] (trivial effect).

The difference in change scores between groups was significant: training group
change (M = 0.58, SD = 0.72) exceeded control group change (M = 0.11, SD = 0.68)
by 0.47 points, t(78) = 2.97, p = .004, d = 0.67, 95% CI [0.22, 1.11]
(medium-to-large effect). The 95% CI for the group difference in change was
[0.15, 0.79].
```

**Improvement**: Adds η²_p (.05), Cohen's d (0.81, 0.16, 0.67), 95% CIs for all effect sizes, SDs for all means, percentage of scale range (8.3%), effect size labels (small/medium/large)

---

### Fix 4: From Missing Descriptives to Complete Table

**❌ BEFORE** (Example 4.3):
```
Training: baseline M = 4.2, post M = 4.8
Control: baseline M = 4.1, post M = 4.2
```

**✅ AFTER** (Example 4.4):
```
Table 1
Descriptive Statistics for Reappraisal and Life Satisfaction by Group and Time

Measure                Group      Time        M      SD    SE    95% CI           n
─────────────────────────────────────────────────────────────────────────────────
Reappraisal (ERQ)      Training   Baseline   4.18   0.82  0.13  [3.92, 4.44]   40
                                  Post       4.76   0.88  0.14  [4.48, 5.04]   40
                                  Change     0.58   0.72  0.11  [0.35, 0.81]   40

                       Control    Baseline   4.12   0.74  0.12  [3.88, 4.36]   40
                                  Post       4.23   0.79  0.13  [3.98, 4.48]   40
                                  Change     0.11   0.68  0.11  [-0.11, 0.33]  40
```

**Improvement**: Complete table with M, SD, SE, 95% CI, n for all Group × Time cells, plus CHANGE scores (critical for intervention studies)

---

### Fix 5: From No Assumptions to Complete Assumption Testing

**❌ BEFORE** (Example 4.3):
```
[Assumptions not mentioned]
```

**✅ AFTER** (Example 4.4):
```
Assumption Testing. Shapiro-Wilk tests confirmed normality for all variables in
both groups at both timepoints (all ps > .05). Levene's tests confirmed homogeneity
of variance for reappraisal (F(1,78) = 1.12, p = .29) and life satisfaction
(F(1,78) = 0.76, p = .39). Mauchly's tests confirmed sphericity for the repeated
measures factor (all ps > .05), so no Greenhouse-Geisser corrections were needed.

Three outliers (>3 SD from group mean) were identified (3.8% of data points).
Following recommendations for small samples (Leys et al., 2013), outliers were
winsorized to 3 SD from the mean. Sensitivity analyses (Supplementary Table S1)
showed that results were unchanged when outliers were excluded entirely,
supporting the robustness of findings.
```

**Improvement**: Reports Shapiro-Wilk (normality), Levene (homogeneity), Mauchly (sphericity) with exact statistics, outlier handling transparent (3 winsorized), sensitivity analysis in Supplement

---

### Fix 6: From No Practical Significance to MID Benchmarks

**❌ BEFORE** (Example 4.3):
```
[Practical significance not addressed]
```

**✅ AFTER** (Example 4.4):
```
Minimal Important Difference Benchmarks. For the SWLS, Diener et al. (1985)
suggest that a 5-point change represents a "noticeable" shift in life satisfaction.
Our training group showed a mean increase of 2.75 points (95% CI [1.43, 4.07]),
which is below this MID threshold. This suggests that while the effect was
statistically significant in exploratory analyses (p = .007), it may not represent
a clinically meaningful improvement in subjective well-being. The upper bound of
the 95% CI (4.07) approaches but does not consistently exceed the MID, indicating
that some participants may have experienced meaningful change while others did not.

Comparison to Meta-Analytic Effects. A recent meta-analysis of cognitive reappraisal
training studies (Malooly et al., 2013) reported a mean effect size of d = 0.71.
Our observed effect (d = 0.67) falls within the 95% CI of this meta-analytic
estimate ([0.52, 0.90]), suggesting our findings are consistent with the broader
literature.
```

**Improvement**: Compares to MID (5.0 for SWLS), meta-analytic d (0.71), notes 2.75 < MID despite statistical significance, provides context for clinical meaningfulness

---

### Fix 7: From Cherry-Picking to Transparent Reporting

**❌ BEFORE** (Example 4.3):
```
Additional analyses revealed that age was positively correlated with baseline
reappraisal use (r = .18, p = .04), suggesting that older participants were
already better at emotion regulation. Gender differences were not significant
(p = .12).

[No mention of other analyses conducted, file-drawer problem]
```

**✅ AFTER** (Example 4.4):
```
Exploratory Analyses. Age showed a weak positive correlation with baseline
reappraisal use (r = .18, p = .11, 95% CI [-0.04, 0.39], r² = .03), accounting
for only 3% of variance. Given the narrow age range in our sample (18-24 years,
M = 19.2, SD = 1.4), this correlation likely has limited generalizability and
may reflect Type I error. Gender was not significantly associated with baseline
reappraisal (r = .12, p = .28, 95% CI [-0.10, 0.34]) or life satisfaction
(r = .08, p = .48, 95% CI [-0.14, 0.30]).

We did not correct for multiple comparisons in these exploratory analyses because
they were not pre-specified hypotheses (Rothman, 1990). However, readers should
consider the inflated family-wise error rate when evaluating these findings. With
4 exploratory tests, the probability of at least one Type I error is approximately
18.5% (1 - (1-.05)^4 = .185). We report all tests conducted to avoid selective
reporting; non-significant results are included in Supplementary Table S2.
```

**Improvement**: Reports ALL tests (not just p<.05), acknowledges FWER (18.5% with 4 tests), provides 95% CIs and r² (3% variance), notes limited generalizability, includes non-significant results in Supplement

---

## Key Lessons for Nature/Science-Level Results

### 1. Effect Sizes are Mandatory, Not Optional

**APA 7th Edition Requirement**: Report effect sizes for ALL statistical tests

**What to report**:
- ANOVA: Partial η²_p (.01 small, .06 medium, .14 large)
- t-tests: Cohen's d (0.2 small, 0.5 medium, 0.8 large)
- Correlations: r with r² (% variance explained)
- Always provide 95% CIs for effect sizes

**Example 4.4 demonstrates**:
```
F(1,78) = 4.35, p = .04, η²_p = .05 (small-to-medium effect)
d = 0.81, 95% CI [0.48, 1.14] (large effect)
r = .18, p = .11, r² = .03 (3% variance)
```

---

### 2. Report Complete Descriptive Statistics in Tables

**For intervention studies**, Table 1 should include:
- Baseline, Post, AND Change scores (all with M, SD, SE, 95% CI)
- Sample sizes per cell (to verify no missing data)
- Clear notes explaining scale ranges and scoring

**Why Change scores matter**:
- Show intervention effect magnitude directly
- Enable within-group vs between-group comparison
- Allow readers to calculate effect sizes independently

**Example 4.4 Table 1**:
- All Group × Time cells (Training/Control × Baseline/Post)
- Change scores with 95% CIs (Training Δ=0.58 [0.35, 0.81])
- Complete stats (M, SD, SE, 95% CI, n) for every cell

---

### 3. Treat p = .052 as Null Result, Not "Trend"

**Never say**: "marginally significant", "trend toward", "approached significance"

**Do say**:
```
"The Group × Time interaction did not reach statistical significance
(F(1,78) = 3.89, p = .052, η²_p = .05). While the direction of effects was
consistent with predictions, this finding should be interpreted with caution
given the p-value exceeded our pre-specified α = .05 threshold."
```

**Add post-hoc power analysis**:
```
"For the observed effect size (η²_p = .05), our sample provided 51% power at
α = .05. To achieve 80% power, we would require N = 146 (n = 73 per group).
This suggests the null result may reflect insufficient power rather than true
absence of an effect, warranting replication with larger samples."
```

---

### 4. Report Assumption Testing Results

**For parametric tests**, report:
- **Normality**: Shapiro-Wilk tests for each group at each timepoint
  - Example 4.4: "all ps > .05"
- **Homogeneity**: Levene's tests with exact F-statistic and p-value
  - Example 4.4: "F(1,78) = 1.12, p = .29"
- **Sphericity**: Mauchly's tests for repeated measures
  - Example 4.4: "all ps > .05, no corrections needed"
- **Outliers**: Detection rule, prevalence, treatment, sensitivity analysis
  - Example 4.4: "3 outliers winsorized, results unchanged in Supplementary Table S1"

**Why this matters**: Shows you checked if analysis was appropriate, allows reviewers to evaluate robustness

---

### 5. Address Practical Significance with MID Benchmarks

**Statistical significance ≠ Practical significance**

**For each significant finding, evaluate**:
1. **Minimal Important Difference (MID)**: Does effect exceed clinical threshold?
   - Example 4.4: SWLS MID = 5.0, observed = 2.75 → below threshold
2. **Meta-analytic comparison**: How does effect compare to prior studies?
   - Example 4.4: d = 0.67 vs meta d = 0.71 → consistent
3. **Percentage of scale range**: What does absolute change mean?
   - Example 4.4: 0.58 on 7-point scale = 8.3% of range
4. **Clinical meaningfulness**: What would this change mean for daily life?
   - Example 4.4: "may not represent a clinically meaningful improvement"

---

### 6. Transparent Reporting of All Analyses

**Avoid selective reporting** (file-drawer problem):
- Report ALL tests conducted, not just p < .05
- Include non-significant results in Supplementary Tables
- Acknowledge family-wise error rate if conducting >1 exploratory test
- State "we report all tests conducted to avoid selective reporting"

**Example 4.4 demonstrates**:
```
"With 4 exploratory tests, the probability of at least one Type I error is
approximately 18.5%. We report all tests conducted to avoid selective reporting;
non-significant results are included in Supplementary Table S2."
```

---

### 7. Mediation Analysis for Mechanism Testing

**If proposing a mechanism**, test it quantitatively:
- Regress outcome on mediator + condition
- Estimate indirect effect with bootstrap CI (5,000 iterations)
- Report percentage of total effect mediated
- Acknowledge if mediation is partial (direct effect remains) vs complete

**Example 4.4 demonstrates**:
```
"The indirect effect through reappraisal change was small but significant
(b = 0.65, SE = 0.31, 95% CI [0.12, 1.35], excluding zero), accounting for 26%
of the total effect. This provides preliminary evidence that increased reappraisal
use may be one mechanism through which training affects well-being, though the
small effect size and wide confidence interval suggest this relationship is
modest and requires replication."
```

---

### 8. Visualize Results with Figures

**Figure 1 should show**:
- Group × Time interaction pattern (line graph or bar graph)
- Error bars representing 95% CIs (not SEM, which is less informative)
- Clear legend and axis labels
- Reference to statistical test in caption

**Example 4.4 Figure 1 description**:
```
"Figure 1. Mean reappraisal use (ERQ scores) and life satisfaction (SWLS scores)
by Group (Training vs Control) and Time (Baseline vs Post-Assessment). Error bars
represent 95% confidence intervals. Panel A shows that the training group exhibited
significantly greater increase in reappraisal use compared to the control group
(Group × Time interaction: F(1,78) = 4.35, p = .04, η²_p = .05)."
```

---

### 9. Acknowledge Limitations Proactively

**Don't wait for Discussion** - address limitations in Results when relevant:
- Self-report vs behavioral measures
- Brief intervention vs multi-session protocols
- Short follow-up vs long-term maintenance
- Sample restrictions (single university, psychology students)
- Alternative explanations not ruled out

**Example 4.4 demonstrates**:
```
"Limitations of Current Findings. First, our measures are self-report questionnaires
assessing beliefs about emotion regulation, not behavioral or physiological
indicators of regulatory success. Second, the brief single-session training format
may produce smaller effects than multi-session interventions. Third, the 2-week
follow-up is relatively short; long-term maintenance is unknown. Fourth, our
sample consisted of undergraduate psychology students at a single university,
limiting generalizability. Finally, we cannot rule out alternative explanations
(e.g., expectancy, attention) without active control manipulation checks."
```

---

### 10. Compare to Meta-Analytic Estimates

**For established research areas**, compare your effect size to:
- Recent meta-analyses (within last 5-10 years)
- 95% CI of meta-analytic estimate
- Heterogeneity across studies (I²)

**Benefits**:
- Contextualizes your finding within broader literature
- Identifies if effect is consistent, smaller, or larger than typical
- Highlights if heterogeneity suggests moderators

**Example 4.4 demonstrates**:
```
"A recent meta-analysis of cognitive reappraisal training studies (Malooly et al.,
2013) reported a mean effect size of d = 0.71 for improvements in emotion regulation
ability. Our observed effect (d = 0.67) falls within the 95% CI of this meta-analytic
estimate ([0.52, 0.90]), suggesting our findings are consistent with the broader
literature."
```

---

## Practice Exercise

### Your Turn: Transform YOUR Results from Weak to Rigorous

**Step 1**: Read your current Results section

**Step 2**: Use Template 4.1 (Bulletproofing Audit) to identify rejection reasons #6-10

**Step 3**: For each problem, write "Before → After" using Example 4.4 as a model

**Example Process**:

```
My Rejection Reason #8 (Statistical Issues): No effect sizes or CIs

BEFORE: "The main effect of condition was significant, F(2,57) = 5.23, p = .008."

AFTER: "The main effect of condition was significant, F(2,57) = 5.23, p = .008,
        η²_p = .16 (large effect). Post-hoc pairwise comparisons with Bonferroni
        correction (α = .017): Condition A (M = 15.2, SD = 3.1, 95% CI [13.8, 16.6])
        differed significantly from Condition B (M = 12.1, SD = 2.8, 95% CI [10.8, 13.4]),
        mean difference = 3.1, 95% CI [1.2, 5.0], d = 1.05, 95% CI [0.52, 1.58],
        p = .003, but not from Condition C (M = 14.0, SD = 3.3, 95% CI [12.5, 15.5]),
        mean difference = 1.2, 95% CI [-0.7, 3.1], d = 0.38, 95% CI [-0.12, 0.88],
        p = .18."

Added: η²_p, d, 95% CIs for M/diff/d, descriptives (M/SD/CI), Bonferroni α
```

**Goal**: Address all 5 Results-specific rejection reasons (#6-10)

---

## Self-Check Questions

Before claiming your Results section is "bulletproof," ask:

1. ❓ **Effect Sizes**: Did I report η²_p (ANOVA), d (t-tests), r (correlations) for EVERY statistical test?
   - If NO → Add effect sizes with 95% CIs and interpretations (small/medium/large)

2. ❓ **Confidence Intervals**: Did I report 95% CIs for all means, mean differences, and effect sizes?
   - If NO → Calculate and report CIs to quantify uncertainty

3. ❓ **Table**: Did I create Table 1 with M, SD, SE, 95% CI, n for all Group × Time cells?
   - If NO → Build complete descriptive statistics table

4. ❓ **Figure**: Did I describe a figure showing interaction pattern with 95% CI error bars?
   - If NO → Create figure description for visual summary

5. ❓ **Assumptions**: Did I report Shapiro-Wilk, Levene, Mauchly tests with exact statistics?
   - If NO → Add assumption testing results section

6. ❓ **Null Results**: Did I treat p > .05 as non-significant (not "marginally significant")?
   - If NO → Rewrite as honest null result + post-hoc power analysis

7. ❓ **Overclaiming**: Did I avoid "prove", "demonstrate", and causal language beyond data?
   - If NO → Replace with qualified language ("preliminary evidence", "suggest", "consistent with")

8. ❓ **Practical Significance**: Did I compare to MID thresholds and meta-analytic estimates?
   - If NO → Add dedicated practical significance section

9. ❓ **Transparent Reporting**: Did I report ALL analyses (including non-significant) to avoid file-drawer?
   - If NO → Add Supplementary Table with all tests, acknowledge FWER

10. ❓ **Limitations**: Did I acknowledge measurement, sample, and design limitations proactively?
    - If NO → Add limitations subsection before summary

**If you answer NO to any question → Your Results section needs revision using Example 4.4 as a template**

---

## Comparison Summary: What Makes Results "Good"?

| Aspect | Bad Results (4.3) | Good Results (4.4) |
|--------|------------------|-------------------|
| **Effect Sizes** | None reported | η²_p, d, r with 95% CIs for ALL tests |
| **Confidence Intervals** | None | 95% CIs for M, diff, d, r |
| **Descriptives** | M only | M, SD, SE, 95% CI, n in Table 1 |
| **Table** | None | Complete Table 1 (all Group × Time cells + change) |
| **Figure** | None | Figure 1 description with 95% CI error bars |
| **Assumptions** | Not reported | Shapiro-Wilk, Levene, Mauchly with exact stats |
| **Outliers** | Not mentioned | 3 winsorized, sensitivity analysis in Supplement |
| **p = .052** | "Marginally significant... trend" | "Did not reach significance" + post-hoc power (51%) |
| **Overclaiming** | "Prove", "demonstrates" | "Preliminary evidence", "suggest" |
| **Practical Significance** | Not addressed | MID benchmarks (5.0 vs 2.75), meta-analysis comparison (d=0.71 vs 0.67) |
| **Multiple Comparisons** | Ignored (4 tests, no correction) | Bonferroni for post-hoc, FWER acknowledged (18.5%) |
| **Mediation** | Not tested | Indirect effect b=0.65, 95% CI [0.12, 1.35], 26% mediation |
| **Limitations** | None in Results | Self-report, brief intervention, short follow-up, confounds acknowledged |
| **Meta-Analysis** | Not compared | d=0.67 within meta 95% CI [0.52, 0.90] |
| **Cherry-Picking** | Only favorable results | All tests in Supp Table S2, null results reported |
| **Top-tier Publishability** | REJECT (5/5 reasons apply) | ACCEPT (all reasons addressed) |

---

## Related Materials

**Templates**:
- `template_4.1_bulletproofing_audit_canvas.md`: AI-assisted audit for Top 10 rejection reasons
- `template_4.2_red_team_blue_team.md`: Game for statistical rigor training
- `template_4.4_peer_review_rubric.md`: Comprehensive peer review rubric

**Examples**:
- `example_4.1_bad_methods.md`: Methods reproducibility gaps (6/30)
- `example_4.2_good_methods.md`: Complete Methods (30/30)
- `example_4.3_bad_results.md`: Results problems (5/5 rejection reasons)
- `example_4.5_red_team_blue_team_case.md`: Game battle case study (coming next)
- `example_4.6_peer_review_model.md`: Model peer review (coming next)

**Lecture Notes**:
- `week4/lecture_notes.md`: Complete bulletproofing strategies (lines 282-422: Results section)

**Recipes**:
- Recipe #38: Overclaiming Detector
- Recipe #39: Statistical Rigor Auditor
- Recipe #40: Transparency Checker

---

**Example Version**: 1.0
**Last Updated**: 2025-01-09
**Use**: Week 4 workshop - Model for Nature/Science-level Results with complete statistical rigor and appropriate claims
